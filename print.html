<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Kafka Sandbox</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded "><a href="quick-start.html"><strong aria-hidden="true">1.</strong> Quick Start</a></li><li class="chapter-item expanded "><a href="what-is-kafka.html"><strong aria-hidden="true">2.</strong> What is Kafka?</a></li><li class="chapter-item expanded "><a href="consuming-and-producing.html"><strong aria-hidden="true">3.</strong> Consuming and Producing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="producing-and-consuming-natives.html"><strong aria-hidden="true">3.1.</strong> Native Consumer and Producer</a></li></ol></li><li class="chapter-item expanded "><a href="message-schemas.html"><strong aria-hidden="true">4.</strong> Message Schemas</a></li><li class="chapter-item expanded "><a href="what-is-kafka-connect.html"><strong aria-hidden="true">5.</strong> What is Kafka Connect?</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="kafka-connect-database-example.html"><strong aria-hidden="true">5.1.</strong> Kafka Connect Database Example</a></li><li class="chapter-item expanded "><a href="kafka-connect-mqtt-example.html"><strong aria-hidden="true">5.2.</strong> Kafka Connect MQTT Example</a></li></ol></li><li class="chapter-item expanded "><a href="performance-tools.html"><strong aria-hidden="true">6.</strong> Performance Tools</a></li><li class="chapter-item expanded "><a href="kafka-proxies.html"><strong aria-hidden="true">7.</strong> Kafka Proxies</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="kafka-rest-proxy.html"><strong aria-hidden="true">7.1.</strong> Kafka Rest Proxy</a></li><li class="chapter-item expanded "><a href="kafka-mqtt-proxy.html"><strong aria-hidden="true">7.2.</strong> Kafka MQTT Proxy</a></li></ol></li><li class="chapter-item expanded "><a href="json-producer-and-consumer.html"><strong aria-hidden="true">8.</strong> JSON Producer and Consumer</a></li><li class="chapter-item expanded "><a href="avro-producer-and-consumer.html"><strong aria-hidden="true">9.</strong> Avro Producer and Consumer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="avro-union.html"><strong aria-hidden="true">9.1.</strong> Avro Union</a></li></ol></li><li class="chapter-item expanded "><a href="spring-boot.html"><strong aria-hidden="true">10.</strong> Spring Boot</a></li><li class="chapter-item expanded "><a href="kafka-streams.html"><strong aria-hidden="true">11.</strong> Kafka Streams</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="interactive-queries.html"><strong aria-hidden="true">11.1.</strong> Interactive Queries</a></li></ol></li><li class="chapter-item expanded "><a href="what-is-ksqldb.html"><strong aria-hidden="true">12.</strong> What is ksqlDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ksqldb-extensions.html"><strong aria-hidden="true">12.1.</strong> ksqlDB Extensions</a></li><li class="chapter-item expanded "><a href="ksqldb-queries.html"><strong aria-hidden="true">12.2.</strong> ksqlDB Queries</a></li><li class="chapter-item expanded "><a href="ksqldb-tests.html"><strong aria-hidden="true">12.3.</strong> ksqlDB Tests</a></li></ol></li><li class="chapter-item expanded "><a href="cleanup.html"><strong aria-hidden="true">13.</strong> Cleanup</a></li><li class="chapter-item expanded affix "><li class="spacer"></li><li class="chapter-item expanded affix "><a href="ports-table.html">Ports Table</a></li><li class="chapter-item expanded affix "><a href="about-this-book.html">About this book</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Kafka Sandbox</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/sauljabin/kafka-sandbox" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><strong>Kafka Sandbox</strong> helps you to deploy a kafka sandbox locally. It intends to be a simple way to get started with kafka and
help you on your learning path. It provides you with a wide variety of tools from the kafka ecosystem and a simple way
to run them all. It also includes a set of tools and tips to make it easier for you to use kafka.
This is not intended to replace official courses.</p>
<div class="warning">
<p>This repository was set up for development and learning purposes.</p>
<p>It does not include security since it is not a production system.</p>
</div>
<h3 id="interesting-links"><a class="header" href="#interesting-links">Interesting Links</a></h3>
<ul>
<li><a href="https://developer.confluent.io/learn-kafka/">Confluent free courses</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/docker/image-reference.html">Confluent docker images references</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/versions-interoperability.html">Confluent versions interoperability</a></li>
</ul>
<h3 id="dependencies"><a class="header" href="#dependencies">Dependencies</a></h3>
<ul>
<li><a href="https://docs.docker.com/engine/install/">docker</a></li>
<li><a href="https://sdkman.io/jdks">java</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>Clone the repo:</p>
<pre><code class="language-bash">git clone https://github.com/sauljabin/kafka-sandbox.git
cd kafka-sandbox
</code></pre>
<p>Run the kafka cluster:</p>
<pre><code class="language-bash">docker compose up -d
</code></pre>
<p>Check running services:</p>
<pre><code class="language-bash">docker compose ps
</code></pre>
<p>Open the sandbox cli:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
<p>Open <a href="https://akhq.io/">AKHQ</a> (a web UI for kafka) at <a href="http://localhost:8080/">http://localhost:8080/</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-kafka"><a class="header" href="#what-is-kafka">What is Kafka?</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/06iRM1Ghr1k"></iframe>
<h3 id="create-a-topic"><a class="header" href="#create-a-topic">Create a Topic</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/kj9JH3ZdsBQ"></iframe>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<pre><code class="language-bash">kafka-topics --create --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic sandbox.test
</code></pre>
<h3 id="list-topics"><a class="header" href="#list-topics">List Topics</a></h3>
<pre><code class="language-bash">kafka-topics --list --bootstrap-server kafka1:9092
</code></pre>
<p>At this point you must understand that the most important concept for you is the partition.
You will be tempted to focus on the topic concept, but you should focus on the partition.
This is due to the very nature of a distributed system.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/y9BStKvVzSs"></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="consuming-and-producing"><a class="header" href="#consuming-and-producing">Consuming and Producing</a></h1>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="produce-messages"><a class="header" href="#produce-messages">Produce Messages</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/I7zm3on_cQQ"></iframe>
<pre><code class="language-bash">kafka-console-producer --bootstrap-server kafka1:9092 \
                       --topic sandbox.test
</code></pre>
<h3 id="consume-messages"><a class="header" href="#consume-messages">Consume Messages</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Z9g4jMQwog0"></iframe>
<pre><code class="language-bash">kafka-console-consumer --from-beginning \
                       --bootstrap-server kafka1:9092 \
                       --group sandbox.test \
                       --topic sandbox.test
</code></pre>
<p>It is key to know that kafka stores binary data, it does not care if
the internal serialization of the data represents a character string,
image, number, or even if the data is encrypted or plain.</p>
<p>All the serialization/deserialization process occurs on the client side,
when producing or consuming.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="producing-and-consuming-natives"><a class="header" href="#producing-and-consuming-natives">Producing and Consuming Natives</a></h1>
<p>Now we are going to develop consumers and producer with java.</p>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="create-all-the-topics"><a class="header" href="#create-all-the-topics">Create All the Topics</a></h3>
<pre><code class="language-bash">for topic in "client.string" "client.integer" "client.long" "client.float" "client.double" "client.boolean" ; do \
kafka-topics --create --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic $topic ; done
</code></pre>
<h3 id="produce"><a class="header" href="#produce">Produce</a></h3>
<p>This code example shows you how to produce a message.
The configuration <code>ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG</code> define what type are we using.</p>
<pre><code class="language-java">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, getSerializer());
KafkaProducer&lt;String, V&gt; producer = new KafkaProducer&lt;&gt;(props);

for (int i = 0; i &lt; messages; i++) {
    V value = createValue();
    ProducerRecord&lt;String, V&gt; record = new ProducerRecord&lt;&gt;(
        topic,
        value
    );
    producer.send(
        record,
        (metadata, exception) -&gt; log.info("Producing message: {}", value)
    );
}
</code></pre>
<p>Then, produce for each type:</p>
<pre><code class="language-bash">for type in "string" "integer" "long" "float" "double" "boolean" ; do \
  gradle kafka-native-clients:run --args="produce client.$type $type 100" ; done
</code></pre>
<h3 id="consume"><a class="header" href="#consume">Consume</a></h3>
<p>It's going to be the same for the consumer, but in this case you have to use the <strong>deserializer</strong>: <code>ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG</code>.</p>
<pre><code class="language-java">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, getDeserializer());
KafkaConsumer&lt;String, V&gt; consumer = new KafkaConsumer&lt;&gt;(props);

ConsumerRecords&lt;String, V&gt; records = consumer.poll(Duration.ofMillis(500));

for (ConsumerRecord&lt;String, V&gt; record : records) {
    log.info("Supplier ID: {}", record.value());
}
</code></pre>
<p>Consume a topic, for example <code>client.string</code>:</p>
<pre><code class="language-bash">gradle kafka-native-clients:run --args="consume client.string string"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="message-schemas"><a class="header" href="#message-schemas">Message Schemas</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2bPx3hfKX04"></iframe>
<p>In this sandbox Schema Registry is already running, we are going to use it in some examples.
Schema Registry will allow you to create applications that share the same data structure when they communicate with each other.</p>
<p>This service could be key for your productions services, especially in a microservices ecosystems.
Although, you will find that evolving a schema could be a problem, particularly when
in your firsts stages of a new services.</p>
<p>It is important to know that Schema Registry is not a plugin or a kafka extension. It is a separated services running
in its own machine.</p>
<p>So, the concept Schema Evolution and Compatibility will become essential for you.
Check this out: <a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/schema-evolution.html">Schema Evolution and Compatibility</a>,
there you will find the <strong>compatibility strategies</strong> for multiple use cases.</p>
<p>Additionally, here you will find a lot of information about Schema Registry: <a href="https://www.confluent.io/blog/schema-registry-for-beginners/">Getting Started with Schemas and Schema Registries</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-kafka-connect"><a class="header" href="#what-is-kafka-connect">What is Kafka Connect?</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/mdsNGujpsF8"></iframe>
<p>It is worth mentioning that just like Schema Registry, Kafka Connect is an external and independent service to Kafka.</p>
<p>Kafka Connect will be very useful when you want to have constant flow of data from a data store (<strong>source</strong>)
to another (<strong>sink</strong>).</p>
<p>Kafka Connect works with plugin, you can find several connector on <a href="https://www.confluent.io/hub/">Confluent Hub</a>.
In this sandbox we have installed 3 plugins:</p>
<ul>
<li><a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc">jdbc connector plugin</a></li>
<li><a href="https://www.confluent.io/hub/confluentinc/kafka-connect-mqtt">mqtt connector plugin</a></li>
</ul>
<p>Plugins above can be found at:</p>
<pre><code class="language-bash">ls kafka-connect/plugins
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-connect-database-example"><a class="header" href="#kafka-connect-database-example">Kafka Connect Database Example</a></h1>
<p>In this example you are going to learn how to move data from a source (<strong>mysql</strong>),
to multiple targets (<strong>postgres and mongo</strong>).</p>
<div class="warning">
<p>This example does not support deletion, for that you have to implement tombstone events at the <a href="https://debezium.io/documentation/reference/connectors/postgresql.html#postgresql-tombstone-events">source</a> and <a href="https://docs.confluent.io/kafka-connect-jdbc/current/sink-connector/index.html#jdbc-sink-delete-mode">sink</a>.</p>
</div>
<h3 id="populate-database"><a class="header" href="#populate-database">Populate Database</a></h3>
<p>Run MySQL and PostgreSQL:</p>
<pre><code class="language-bash">docker compose --profile sql up -d
</code></pre>
<p>Then open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
<p>Populate it:</p>
<pre><code class="language-bash">mysql --host=mysql --database=sandbox &lt; kafka-connect/sql/customers.sql
</code></pre>
<p>That command should have created the table <code>customers</code> and inserted 200 records.</p>
<p>Now you can open <a href="http://localhost:9090">Adminer</a> or run:</p>
<pre><code class="language-bash">mysql --host=mysql --database=sandbox -e "select * from customers"
</code></pre>
<h3 id="create-source-connector"><a class="header" href="#create-source-connector">Create Source Connector</a></h3>
<p>Check the installed plugins:</p>
<pre><code class="language-bash">http kafka-connect:8083/connector-plugins
</code></pre>
<p>Now you have to hit the kafka connect rest service to create a new source, next you have the rest payload:</p>
<pre><code class="language-json">{
  "name": "mysql-source",
  "config": {
    "tasks.max": "1",
    "table.whitelist": "customers",
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:mysql://mysql:3306/sandbox",
    "connection.user": "root",
    "connection.password": "notasecret",
    "mode": "timestamp",
    "timestamp.column.name": "created",
    "topic.prefix": "connect.",
    "transforms": "createKey",
    "transforms.createKey.type": "org.apache.kafka.connect.transforms.ValueToKey",
    "transforms.createKey.fields": "id"
  }
}
</code></pre>
<p>Create the connector using the API:</p>
<pre><code class="language-bash">http kafka-connect:8083/connectors &lt; kafka-connect/requests/create-connector-mysql-source.json
</code></pre>
<p>If you open <a href="http://localhost:8080">AKHQ</a> you should see a new topic: <code>connect.customers</code>.</p>
<h3 id="create-sink-connector"><a class="header" href="#create-sink-connector">Create Sink Connector</a></h3>
<p>Payload:</p>
<pre><code class="language-json">{
  "name": "postgres-sink",
  "config": {
    "tasks.max": "1",
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "connection.url": "jdbc:postgresql://postgres:5432/sandbox",
    "connection.user": "postgres",
    "connection.password": "notasecret",
    "delete.enabled": false,
    "pk.mode": "record_key",
    "pk.key": "id",
    "insert.mode": "upsert",
    "auto.create": true,
    "topics": "connect.customers",
    "transforms": "dropPrefix",
    "transforms.dropPrefix.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.dropPrefix.regex": "connect\\.(.*)",
    "transforms.dropPrefix.replacement": "$1"
  }
}
</code></pre>
<p>Create sink connector:</p>
<pre><code class="language-bash">http kafka-connect:8083/connectors &lt; kafka-connect/requests/create-connector-postgres-sink.json
</code></pre>
<p>This sink connector is going to create a table <code>customers</code> on postgres and insert all records.</p>
<p>Now you can open <a href="http://localhost:9090">Adminer</a> or run:</p>
<pre><code class="language-bash">psql --host=postgres --user=postgres --dbname=sandbox -c "select * from customers"
</code></pre>
<p>List connector:</p>
<pre><code class="language-bash">http kafka-connect:8083/connectors
</code></pre>
<h3 id="deleting-connectors"><a class="header" href="#deleting-connectors">Deleting Connectors</a></h3>
<pre><code class="language-bash">http DELETE kafka-connect:8083/connectors/postgres-sink
http DELETE kafka-connect:8083/connectors/mysql-source
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-connect-mqtt-example"><a class="header" href="#kafka-connect-mqtt-example">Kafka Connect MQTT Example</a></h1>
<h3 id="setup-broker"><a class="header" href="#setup-broker">Setup Broker</a></h3>
<p>Start MQTT server:</p>
<pre><code class="language-bash">docker compose --profile mqtt up -d
</code></pre>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<p>In one terminal, subscribe to mqtt topics:</p>
<pre><code class="language-bash">mosquitto_sub -h mosquitto -t 'house/+/brightness'
</code></pre>
<p>In another terminal, publish messages:</p>
<pre><code class="language-bash">mosquitto_pub -h mosquitto -t 'house/room/brightness' -m '800LM'
mosquitto_pub -h mosquitto -t 'house/kitchen/brightness' -m '1000LM'
</code></pre>
<h3 id="create-source-connector-1"><a class="header" href="#create-source-connector-1">Create Source Connector</a></h3>
<p>Payload:</p>
<pre><code class="language-json">{
  "name": "mqtt-source",
  "config": {
      "connector.class": "io.confluent.connect.mqtt.MqttSourceConnector",
      "tasks.max": "1",
      "mqtt.server.uri": "tcp://mosquitto:1883",
      "mqtt.topics":"house/+/brightness",
      "kafka.topic":"connect.brightness",
      "mqtt.qos": "2",
      "confluent.topic.bootstrap.servers": "kafka1:9092",
      "confluent.topic.replication.factor": "1"
  }
}
</code></pre>
<p>Create a connector using the API:</p>
<pre><code class="language-bash">http kafka-connect:8083/connectors &lt; kafka-connect/requests/create-connector-mqtt-source.json
</code></pre>
<p>In one terminal, consume from kafka:</p>
<pre><code class="language-bash">kafka-console-consumer --from-beginning --group connect.mqtt \
                       --topic connect.brightness  \
                       --bootstrap-server kafka1:9092 \
                       --property print.key=true
</code></pre>
<p>In another terminal, publish new messages to the MQTT broker:</p>
<pre><code class="language-bash">mosquitto_pub -h mosquitto -t 'house/room/brightness' -m '810LM'
mosquitto_pub -h mosquitto -t 'house/kitchen/brightness' -m '1020LM'
</code></pre>
<p>Deleting the connector:</p>
<pre><code class="language-bash">http DELETE kafka-connect:8083/connectors/mqtt-source
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance-tools"><a class="header" href="#performance-tools">Performance Tools</a></h1>
<p>Performance tuning involves two important metrics:</p>
<ul>
<li>Latency measures how long it takes to process one event.</li>
<li>Throughput measures how many events arrive within a specific amount of time.</li>
</ul>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<p>Run help:</p>
<pre><code class="language-bash">kafka-producer-perf-test --help
kafka-consumer-perf-test --help
</code></pre>
<p>Create a topic:</p>
<pre><code class="language-bash">kafka-topics --create \
             --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic sandbox.performance
</code></pre>
<h3 id="performance-tests"><a class="header" href="#performance-tests">Performance Tests</a></h3>
<p>Test
producer (<a href="https://docs.confluent.io/kafka/operations-tools/kafka-tools.html#kafka-producer-perf-test-sh">confluent doc</a>):</p>
<pre><code class="language-bash">kafka-producer-perf-test --topic sandbox.performance \
                         --throughput -1 \
                         --num-records 3000000 \
                         --record-size 1024 \
                         --producer-props acks=all bootstrap.servers=kafka1:9092
</code></pre>
<ul>
<li>Throughput in MB/sec.</li>
<li>Latency in milliseconds.</li>
</ul>
<p>Test
consumer (<a href="https://docs.confluent.io/kafka/operations-tools/kafka-tools.html#kafka-consumer-perf-test-sh">confluent doc</a>):</p>
<pre><code class="language-bash">kafka-consumer-perf-test --topic sandbox.performance \
                         --bootstrap-server kafka1:9092 \
                         --messages 3000000
</code></pre>
<ul>
<li><code>start.time, end.time</code>: shows test start and end time.</li>
<li><code>data.consumed.in.MB</code>: shows the size of all messages consumed.</li>
<li><code>MB.sec</code>: shows how much data transferred in megabytes per second (Throughput on size).</li>
<li><code>data.consumed.in.nMsg</code>: shows the count of the total messages consumed during this test.</li>
<li><code>nMsg.sec</code>: shows how many messages were consumed in a second (Throughput on the count of messages).</li>
</ul>
<p>Test end to end
latency (<a href="https://docs.confluent.io/kafka/operations-tools/kafka-tools.html#kafka-e2e-latency-sh">confluent doc</a>):</p>
<pre><code class="language-bash">kafka-e2e-latency kafka1:9092 sandbox.performance 10000 all 1024
</code></pre>
<p>Following are the required arguments:</p>
<ul>
<li><code>broker_list</code>: The location of the bootstrap broker for both the producer and the consumer.</li>
<li><code>topic</code>: The topic name used by both the producer and the consumer to send/receive messages.</li>
<li><code>num_messages</code>: The number of messages to send</li>
<li><code>producer_acks</code>: The producer setting for acks.</li>
<li><code>message_size_bytes</code>: size of each message in bytes.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-proxies"><a class="header" href="#kafka-proxies">Kafka Proxies</a></h1>
<p>The kafka proxies are tools that simplify the communication between a component and kafka through a non-kafka protocol.</p>
<p>Confluent provides a couple of proxies. It is important to mention that these tools are used when the use case is simple
enough. Generally when you need a lot of control over clients, using proxies is not recommended.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-rest-proxy"><a class="header" href="#kafka-rest-proxy">Kafka REST Proxy</a></h1>
<p>The Kafka REST Proxy provides a RESTful interface to a Kafka cluster.
Use this when you really need a rest interface since it is usually more complex than using conventional kafka clients.
You can check the API reference <a href="https://docs.confluent.io/platform/current/kafka-rest/api.html">here</a>.</p>
<h3 id="setup"><a class="header" href="#setup">Setup</a></h3>
<p>Run Kafka REST Proxy:</p>
<pre><code class="language-bash">docker compose --profile proxies up -d
</code></pre>
<p>Then open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
<p>Check the cluster information:</p>
<pre><code class="language-bash">http kafka-rest:8082/v3/clusters
</code></pre>
<h3 id="create-topic"><a class="header" href="#create-topic">Create Topic</a></h3>
<p>Payload:</p>
<pre><code class="language-json">{
  "topic_name": "proxy.rest",
  "partitions_count": 3,
  "replication_factor": 3
}
</code></pre>
<p>Hit rest proxy:</p>
<pre><code class="language-bash">http kafka-rest:8082/v3/clusters/${CLUSTER_ID}/topics &lt; kafka-rest/requests/create-topic.json
</code></pre>
<p>List topics:</p>
<pre><code class="language-bash">http kafka-rest:8082/v3/clusters/${CLUSTER_ID}/topics
</code></pre>
<h3 id="produce-1"><a class="header" href="#produce-1">Produce</a></h3>
<p>Payload:</p>
<pre><code class="language-json">{
  "key": {
    "type": "STRING",
    "data": "ce59e9a6-aafd-44f4-bdc6-f285adfcc836"
  },
  "value": {
    "type": "AVRO",
    "schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]}",
    "data": {
      "name": "John Doe"
    }
  }
}
</code></pre>
<p>Send payload:</p>
<pre><code class="language-bash">http kafka-rest:8082/v3/clusters/${CLUSTER_ID}/topics/proxy.rest/records &lt; kafka-rest/requests/produce-avro-message.json
</code></pre>
<h3 id="consume-1"><a class="header" href="#consume-1">Consume</a></h3>
<pre><code class="language-bash">kafka-avro-console-consumer --bootstrap-server kafka1:9092 \
        --topic proxy.rest \
        --from-beginning \
        --property schema.registry.url=http://schema-registry:8081
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-mqtt-proxy"><a class="header" href="#kafka-mqtt-proxy">Kafka MQTT Proxy</a></h1>
<p>MQTT Proxy enables MQTT clients to use the MQTT protocol to publish data directly to Apache Kafka.
This does not convert kafka into a MQTT broker, this aims to provide a simple way to publish/persist IoT data to Kafka.</p>
<h3 id="setup-1"><a class="header" href="#setup-1">Setup</a></h3>
<p>Run Kafka MQTT Proxy:</p>
<pre><code class="language-bash">docker compose --profile proxies up -d
</code></pre>
<p>Then open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
<h3 id="publish-messages"><a class="header" href="#publish-messages">Publish Messages</a></h3>
<p>Create topic:</p>
<pre><code class="language-bash">kafka-topics --create \
             --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic proxy.mqtt
</code></pre>
<p>Publish using mqtt proxy:</p>
<pre><code class="language-bash">mosquitto_pub -h kafka-mqtt -p 1884 -t 'house/room/temperature' -m '20C'
</code></pre>
<p>Check the data:</p>
<pre><code class="language-bash">kafka-console-consumer --from-beginning \
                       --bootstrap-server kafka1:9092 \
                       --group proxy.mqtt \
                       --topic proxy.mqtt  \
                       --property print.key=true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="json-producer-and-consumer"><a class="header" href="#json-producer-and-consumer">JSON Producer and Consumer</a></h1>
<p>This example shows you how to use json and schema registry for producing and consuming.</p>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="other-links"><a class="header" href="#other-links">Other Links</a></h3>
<ul>
<li><a href="https://json-schema.org/">json schema</a></li>
<li><a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/serdes-json.html">confluent example</a></li>
</ul>
<h3 id="pojo"><a class="header" href="#pojo">POJO</a></h3>
<p>The first step is to create the structure:</p>
<pre><code class="language-java">package kafka.sandbox.cli;

import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class User {
    @JsonProperty
    public String id;

    @JsonProperty
    public String firstName;

    @JsonProperty
    public String lastName;

    @JsonProperty
    public String address;

    @JsonProperty
    public int age;
}
</code></pre>
<p>Confluent java library uses <a href="https://github.com/FasterXML/jackson-annotations?tab=readme-ov-file#usage-simple">jackson annotations</a>.</p>
<h3 id="configurations"><a class="header" href="#configurations">Configurations</a></h3>
<p>It is possible to produce with or without Schema Registry. It'll depend on the configurations.</p>
<p>Producer:</p>
<pre><code class="language-java">if (useSchemaRegistry) {
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaJsonSchemaSerializer.class);
    props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://schema-registry:8081");
} else {
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaJsonSerializer.class);
}
</code></pre>
<p>Consumer:</p>
<pre><code class="language-java">if (useSchemaRegistry) {
    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaJsonSchemaDeserializer.class);
    props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://schema-registry:8081");
} else {
    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaJsonDeserializer.class);
}
</code></pre>
<h3 id="setup-2"><a class="header" href="#setup-2">Setup</a></h3>
<p>Create a topic to produce json <strong>without</strong> Schema Registry:</p>
<pre><code class="language-bash">kafka-topics --create --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic client.users
</code></pre>
<p>Create a topic to produce json <strong>with</strong> Schema Registry:</p>
<pre><code class="language-bash">kafka-topics --create --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic client.schema.users
</code></pre>
<h3 id="produce-2"><a class="header" href="#produce-2">Produce</a></h3>
<p>Produce <strong>without</strong> Schema Registry:</p>
<pre><code class="language-bash">gradle kafka-json-clients:run --args="produce client.users 100"
</code></pre>
<p>Produce <strong>with</strong> Schema Registry:</p>
<pre><code class="language-bash">gradle kafka-json-clients:run --args="produce -s client.schema.users 100"
</code></pre>
<h3 id="consume-2"><a class="header" href="#consume-2">Consume</a></h3>
<p>Consume <strong>without</strong> Schema Registry:</p>
<pre><code class="language-bash">gradle kafka-json-clients:run --args="consume client.users"
</code></pre>
<p>Consume <strong>with</strong> Schema Registry:</p>
<pre><code class="language-bash">gradle kafka-json-clients:run --args="consume -s client.schema.users"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="avro-producer-and-consumer"><a class="header" href="#avro-producer-and-consumer">Avro Producer and Consumer</a></h1>
<p>These examples produce and consume messages from the <code>supplier</code> topic. The producer example produces random suppliers.</p>
<p>The goal is to communicate producers and consumers with avro serialized messages.</p>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="other-links-1"><a class="header" href="#other-links-1">Other Links</a></h3>
<ul>
<li><a href="https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/serdes-avro.html">confluent avro producer and consumer examples</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html">kafka consumer settings</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html">kafka producer settings</a></li>
</ul>
<h3 id="avro-schema"><a class="header" href="#avro-schema">Avro Schema</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/SZX9DM_gyOE"></iframe>
<p>Avro allows us to serialize/deserialize messages. Here it is our example
avro schema:</p>
<pre><code class="language-json">{
  "type": "record",
  "name": "Supplier",
  "namespace": "kafka.sandbox.avro",
  "version": "1",
  "fields": [
    {
      "name": "id",
      "type": "string"
    },
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "address",
      "type": "string"
    },
    {
      "name": "country",
      "type": "string"
    }
  ]
}
</code></pre>
<h3 id="setup-3"><a class="header" href="#setup-3">Setup</a></h3>
<p>Create a topic:</p>
<pre><code class="language-bash">kafka-topics --create --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic client.suppliers
</code></pre>
<h3 id="produce-3"><a class="header" href="#produce-3">Produce</a></h3>
<p>As you can see now we are using the autogenerated class <code>Supplier</code>.</p>
<pre><code class="language-java">KafkaProducer&lt;String, Supplier&gt; producer = new KafkaProducer&lt;&gt;(props);

for (int i = 0; i &lt; messages; i++) {
    Supplier supplier = createNew();
    ProducerRecord&lt;String, Supplier&gt; record = new ProducerRecord&lt;&gt;(
        topic,
        supplier.getId().toString(),
        supplier
    );
    producer.send(
        record,
        (metadata, exception) -&gt; log.info("Producing message: {}", supplier)
    );
}
</code></pre>
<pre><code class="language-bash">gradle kafka-avro-clients:run --args="produce client.suppliers 100"
</code></pre>
<h3 id="consume-3"><a class="header" href="#consume-3">Consume</a></h3>
<p>It is the same for the consumer, we are using the avro class <code>Supplier</code>.</p>
<pre><code class="language-java">ConsumerRecords&lt;String, Supplier&gt; records = consumer.poll(Duration.ofMillis(500));

for (ConsumerRecord&lt;String, Supplier&gt; record : records) {
    log.info("Supplier ID: {}", record.key());
}
</code></pre>
<pre><code class="language-bash">gradle kafka-avro-clients:run --args="consume client.suppliers"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="avro-union"><a class="header" href="#avro-union">Avro Union</a></h1>
<p>These example show you how to use <a href="https://avro.apache.org/docs/1.10.2/spec.html#Unions">Unions</a>.</p>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="avro-schema-1"><a class="header" href="#avro-schema-1">Avro Schema</a></h3>
<p>In this schema we create a field <code>metric</code> that can be a <code>TimerMetric</code> or <code>CounterMetric</code>.</p>
<pre><code class="language-json">{
  "type": "record",
  "name": "Metric",
  "namespace": "kafka.sandbox.avro",
  "version": "1",
  "fields": [
    {
      "name": "metricId",
      "type": "string"
    },
    {
      "name": "metricType",
      "type": {
        "type": "enum",
        "name": "MetricType",
        "symbols": [
          "TIMER",
          "COUNTER"
        ],
        "default": "TIMER"
      }
    },
    {
      "name": "metric",
      "type": [
        {
          "type": "record",
          "name": "TimerMetric",
          "namespace": "kafka.sandbox.avro",
          "version": "1",
          "fields": [
            {
              "name": "avg",
              "type": "double"
            }
          ]
        },
        {
          "type": "record",
          "name": "CounterMetric",
          "namespace": "kafka.sandbox.avro",
          "version": "1",
          "fields": [
            {
              "name": "count",
              "type": "long"
            }
          ]
        }
      ]
    }
  ]
}
</code></pre>
<p>The <code>Metric</code> java class will define an object (for the <code>metric</code> field) instead of a specific type (<code>TimerMetric</code> or <code>CounterMetric</code>).</p>
<pre><code class="language-java">private java.lang.Object metric;
</code></pre>
<h3 id="setup-4"><a class="header" href="#setup-4">Setup</a></h3>
<p>Create a topic:</p>
<pre><code class="language-bash">kafka-topics --create --bootstrap-server kafka1:9092 \
             --replication-factor 3 \
             --partitions 3 \
             --topic client.metrics
</code></pre>
<h3 id="produce-4"><a class="header" href="#produce-4">Produce</a></h3>
<pre><code class="language-bash">gradle kafka-avro-union-clients:run --args="produce client.metrics 100"
</code></pre>
<h3 id="consume-4"><a class="header" href="#consume-4">Consume</a></h3>
<pre><code class="language-bash">gradle kafka-avro-union-clients:run --args="consume client.metrics"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spring-boot"><a class="header" href="#spring-boot">Spring Boot</a></h1>
<p>Spring Boot + Spring Kafka producer and consumer examples.</p>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="other-links-2"><a class="header" href="#other-links-2">Other LInks</a></h3>
<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-spring-boot-application/">confluent spring kafka examples</a></li>
<li><a href="https://docs.spring.io/spring-kafka/reference/html/">spring kafka settings</a></li>
</ul>
<h3 id="setup-5"><a class="header" href="#setup-5">Setup</a></h3>
<p>Run spring boot:</p>
<pre><code class="language-bash">gradle kafka-spring-boot:bootRun
</code></pre>
<h3 id="produce-5"><a class="header" href="#produce-5">Produce</a></h3>
<p>Spring has the class <code>KafkaTemplate</code> that allows you to produce messages.</p>
<pre><code class="language-java">@Value("${spring.kafka.topic}")
private String topic;

@Autowired
private KafkaTemplate&lt;String, Customer&gt; kafkaTemplate;

public void sendCustomer(Customer customer) {
    log.info("Producing message: {}", customer);
    kafkaTemplate.send(topic, customer.getId().toString(), customer);
}
</code></pre>
<p>In another terminal:</p>
<pre><code class="language-bash">http :8585/produce messages==10
</code></pre>
<h3 id="consume-5"><a class="header" href="#consume-5">Consume</a></h3>
<p>You can use the <code>KafkaListener</code> annotation.</p>
<pre><code class="language-java">@KafkaListener(topics = { "${spring.kafka.topic}" })
public void consume(ConsumerRecord&lt;String, Customer&gt; record) {
    log.info("Customer ID: {}", record.value());
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-streams"><a class="header" href="#kafka-streams">Kafka Streams</a></h1>
<p>Kafka Streams is a client library providing organizations with a particularly efficient framework for processing
streaming data. It offers a streamlined method for creating applications and microservices that must process data in
real-time to be effective.</p>
<ul>
<li><a href="https://github.com/confluentinc/kafka-streams-examples">kafka streams examples</a></li>
<li>more kafka streams examples <a href="https://github.com/sauljabin/kafka-streams-sandbox">here</a>.</li>
<li><a href="https://docs.confluent.io/platform/current/streams/architecture.html">kafka stream architecture</a></li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/y9a3fldlvnI"></iframe>
<p>In this example we are going to create a data processing pipeline (topology), which
is going to count the suppliers by country (check the <a href="avro-producer-and-consumer.html">Kafka Clients - Avro Producer and Consumer</a> section).
So, kafka stream is going to consume form the <code>client.suppliers</code> topic, then is
going to process the data and finally is going to publish the results in the
topic <code>streams.results</code>.</p>
<pre><code class="language-java">// read from suppliers topic
KStream&lt;String, Supplier&gt; suppliers = builder.stream(source);

// aggregate the new supplier counts by country
KTable&lt;String, Long&gt; aggregated = suppliers
        // map the country as key
        .map((key, value) -&gt; new KeyValue&lt;&gt;(value.getCountry().toString(), value))
        .groupByKey()
        // aggregate and materialize the store
        .count(Materialized.as("SupplierCountByCountry"));

// write the results to a topic
aggregated.toStream()
        // print results
        .peek((key, value) -&gt; log.info("Country = {}, Total supplier counts = {}", key, value))
        // publish results
        .to(sink, Produced.with(Serdes.String(), Serdes.Long()));

// build the topology
Topology topology = builder.build();
</code></pre>
<h3 id="run-topology"><a class="header" href="#run-topology">Run Topology</a></h3>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<pre><code class="language-bash">gradle kafka-streams:run --args="streams client.suppliers streams.results"
</code></pre>
<p>Print results (in another terminal):</p>
<pre><code class="language-bash">kafka-console-consumer --from-beginning --group kafka-streams.consumer \
                       --topic streams.results  \
                       -bootstrap-server kafka1:9092 \
                       --property print.key=true \
                       --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
</code></pre>
<p>Send new suppliers (in another terminal):</p>
<pre><code class="language-bash">gradle kafka-avro-clients:run --args="produce client.suppliers 100"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interactive-queries"><a class="header" href="#interactive-queries">Interactive Queries</a></h1>
<p>Interactive Queries allow you to leverage the state of your application from outside your application. The Kafka Streams API enables your applications to be queryable.</p>
<ul>
<li><a href="https://docs.confluent.io/platform/current/streams/developer-guide/interactive-queries.html">interactive queries</a></li>
</ul>
<p>This example is using <a href="https://grpc.io/">gRPC</a> to request queries to the kafka stream server.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fVDdY36Wk3w"></iframe>
<h3 id="query-results"><a class="header" href="#query-results">Query Results</a></h3>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<p>Having kafka streams running (see <a href="kafka-streams.html">Kafka Streams</a>),
you can query form the terminal a specific country, example:</p>
<pre><code class="language-bash">gradle -q kafka-streams:run --args="count Ecuador"
</code></pre>
<p>This is possible because we have access to the kafka streams stores in runtime.
This way we can query over the store a return and response.</p>
<p>Check the gRPC server:</p>
<pre><code class="language-java">ReadOnlyKeyValueStore&lt;String, Long&gt; keyValueStore = streams.store(
        StoreQueryParameters.fromNameAndType("SupplierCountByCountry", QueryableStoreTypes.keyValueStore()));

String country = request.getName();
Long total = keyValueStore.get(country);
String value = String.format("Country: %s, Total Suppliers: %s", country, total != null ? total : 0);
CountReply reply = CountReply.newBuilder().setMessage(value).build();
responseObserver.onNext(reply);
</code></pre>
<p>Check the gRPC client:</p>
<pre><code class="language-java">ManagedChannel channel = Grpc.newChannelBuilder("localhost:5050", InsecureChannelCredentials.create())
                .build();

CounterServiceBlockingStub blockingStub = CounterServiceGrpc.newBlockingStub(channel);
CountReply countByCountry = blockingStub.getCountByCountry(CountRequest.newBuilder().setName(country).build());
System.out.println(countByCountry.getMessage());
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-ksqldb"><a class="header" href="#what-is-ksqldb">What is ksqlDB?</a></h1>
<p>ksqlDB is a database that's purpose-built for stream processing applications.
ksqlDB it is not a SQL database, it provides an extra layer for implementing kstream, ktable and connectors through a language (ksql) based on sql.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ji7YMlJUqsA"></iframe>
<ul>
<li><a href="https://ksqldb.io/">ksqldb</a></li>
<li><a href="https://docs.ksqldb.io/en/latest/reference/server-configuration/">ksqldb settings</a></li>
</ul>
<h3 id="run-ksqldb"><a class="header" href="#run-ksqldb">Run ksqlDB</a></h3>
<p>Start ksqldb:</p>
<pre><code class="language-bash">docker compose --profile ksqldb up -d
</code></pre>
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
<p>After a few seconds check if it is up:</p>
<pre><code class="language-bash">http ksqldb:8088/info
</code></pre>
<p>One line shell interaction:</p>
<pre><code class="language-bash">ksql -e "SHOW STREAMS;" http://ksqldb:8088
</code></pre>
<p>Interactive ksqlDB shell:</p>
<pre><code class="language-bash">ksql http://ksqldb:8088
</code></pre>
<p>Then enter <code>SHOW STREAMS;</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ksqldb-extensions"><a class="header" href="#ksqldb-extensions">ksqlDB Extensions</a></h1>
<p>ksqlDB extensions are pieces of logic for transforming or aggregating events that ksqlDB can't currently express.</p>
<ul>
<li><a href="https://docs.ksqldb.io/en/latest/how-to-guides/create-a-user-defined-function">ksqldb extensions (udf, udtf, udaf)</a></li>
</ul>
<p>Check the <a href="what-is-ksqldb.html">Kafka ksqlDB</a> section.</p>
<p>For creating the <code>jar</code> extension, you can use the following command (development purposes):</p>
<pre><code class="language-bash">gradle kafka-ksqldb-extensions:shadowJar
</code></pre>
<h2 id="custom-udf"><a class="header" href="#custom-udf">Custom UDF</a></h2>
<pre><code class="language-java">package kafka.sandbox.ksqldb;

import io.confluent.ksql.function.udf.Udf;
import io.confluent.ksql.function.udf.UdfDescription;
import io.confluent.ksql.function.udf.UdfParameter;

@UdfDescription(
    name = "taxes",
    author = "kafka sandbox",
    version = "1.0.0",
    description = "A custom taxes formula for orders."
)
public class TaxesUdf {

    public static final double TAXES = .12;

    @Udf(description = "Calculate taxes.")
    public double taxes(@UdfParameter double amount) {
        return amount * TAXES;
    }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ksqldb-queries"><a class="header" href="#ksqldb-queries">ksqlDB Queries</a></h1>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="create-a-stream"><a class="header" href="#create-a-stream">Create a Stream</a></h3>
<p>Create orders stream:</p>
<pre><code class="language-bash">ksql -f kafka-ksqldb/ksql/create-orders.ksql http://ksqldb:8088
</code></pre>
<p>The previous command executed this:</p>
<pre><code class="language-sql">CREATE STREAM orders (orderId INT KEY, orderUnits INT, totalAmount DOUBLE)
  WITH (kafka_topic='ksqldb.orders', key_format='json', value_format='json', partitions=10, replicas=3);

CREATE STREAM orderSizes 
  WITH (kafka_topic='ksqldb.order_sizes', key_format='json', value_format='json', partitions=10, replicas=3)
  AS SELECT
    orderId,
    orderUnits,
    totalAmount,
    CASE WHEN orderUnits &lt; 2 THEN 'small' WHEN orderUnits &lt; 4 THEN 'medium' ELSE 'large' END AS orderSize,
    taxes(totalAmount) AS tax
  FROM orders EMIT CHANGES;
</code></pre>
<p>List of streams:</p>
<pre><code class="language-bash">ksql -e "SHOW STREAMS;" http://ksqldb:8088
</code></pre>
<h3 id="insert"><a class="header" href="#insert">Insert</a></h3>
<p>As any other SQL interpreter, ksqlDB will use the command <code>INSERT</code> to populate a table.</p>
<pre><code class="language-sql">INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000000, 2, 100.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000001, 4, 200.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000002, 6, 300.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000003, 3, 150.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000004, 1, 50.0);
</code></pre>
<p>Insert orders:</p>
<pre><code class="language-bash">ksql -f kafka-ksqldb/ksql/insert-orders.ksql http://ksqldb:8088
</code></pre>
<h3 id="show"><a class="header" href="#show">Show</a></h3>
<pre><code class="language-bash">ksql -e "PRINT 'ksqldb.order_sizes' FROM BEGINNING;" http://ksqldb:8088
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ksqldb-tests"><a class="header" href="#ksqldb-tests">ksqlDB Tests</a></h1>
<ul>
<li><a href="https://docs.ksqldb.io/en/latest/how-to-guides/test-an-app/">ksqldb test runner</a></li>
</ul>
<p>One interesting feature that ksqlDB has is the test runner, it allows you to test a query before deploying it.</p>
<div class="warning">
<p>Open a terminal inside the sandbox environment:</p>
<pre><code class="language-bash">docker compose exec cli bash
</code></pre>
</div>
<h3 id="run-a-test"><a class="header" href="#run-a-test">Run a Test</a></h3>
<pre><code class="language-bash">ksql-test-runner -e kafka-ksqldb-extensions/extensions/ \
        -s kafka-ksqldb/ksql/create-orders.ksql \
        -i kafka-ksqldb/tests/orders-input.json \
        -o kafka-ksqldb/tests/orders-output.json | grep '&gt;&gt;&gt;'
</code></pre>
<h2 id="tests"><a class="header" href="#tests">Tests</a></h2>
<p>A test has 2 parts, inputs and outputs, the ksqldb test runner will compare then to define if
the test passes or fails.</p>
<h4 id="orders-inputjson"><a class="header" href="#orders-inputjson">orders-input.json</a></h4>
<pre><code class="language-json">{
  "inputs": [
    {"topic": "ksqldb.orders", "timestamp": 0, "key": 1000000, "value": {"orderUnits": 2, "totalAmount": 100.0}},
    {"topic": "ksqldb.orders", "timestamp": 0, "key": 1000001, "value": {"orderUnits": 4, "totalAmount": 200.0}},
    {"topic": "ksqldb.orders", "timestamp": 0, "key": 1000002, "value": {"orderUnits": 6, "totalAmount": 300.0}},
    {"topic": "ksqldb.orders", "timestamp": 0, "key": 1000003, "value": {"orderUnits": 3, "totalAmount": 150.0}},
    {"topic": "ksqldb.orders", "timestamp": 0, "key": 1000004, "value": {"orderUnits": 1, "totalAmount": 50.0}}
  ]
}
</code></pre>
<h4 id="orders-outputjson"><a class="header" href="#orders-outputjson">orders-output.json</a></h4>
<pre><code class="language-json">{
  "outputs": [
    {"topic": "ksqldb.order_sizes", "timestamp": 0, "key": 1000000, "value": {"ORDERUNITS": 2, "TOTALAMOUNT": 100.0, "ORDERSIZE": "medium", "TAX": 12.0}},
    {"topic": "ksqldb.order_sizes", "timestamp": 0, "key": 1000001, "value": {"ORDERUNITS": 4, "TOTALAMOUNT": 200.0, "ORDERSIZE": "large" , "TAX": 24.0}},
    {"topic": "ksqldb.order_sizes", "timestamp": 0, "key": 1000002, "value": {"ORDERUNITS": 6, "TOTALAMOUNT": 300.0, "ORDERSIZE": "large" , "TAX": 36.0}},
    {"topic": "ksqldb.order_sizes", "timestamp": 0, "key": 1000003, "value": {"ORDERUNITS": 3, "TOTALAMOUNT": 150.0, "ORDERSIZE": "medium", "TAX": 18.0}},
    {"topic": "ksqldb.order_sizes", "timestamp": 0, "key": 1000004, "value": {"ORDERUNITS": 1, "TOTALAMOUNT": 50.0, "ORDERSIZE": "small" , "TAX": 6.0}}
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cleanup"><a class="header" href="#cleanup">Cleanup</a></h1>
<p>Shutting down all services:</p>
<pre><code class="language-bash">docker compose --profile proxies --profile sql --profile mqtt --profile ksqldb down
</code></pre>
<div class="warning">
<p>If you want to remove the data pass <code>-v</code> at the end of the previous command.</p>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="ports"><a class="header" href="#ports">Ports</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Service</th><th>Port</th></tr></thead><tbody>
<tr><td><a href="http://localhost:8080/">AKHQ Kafka UI</a></td><td>8080</td></tr>
<tr><td><a href="http://localhost:9090/">Adminer SQL UI</a></td><td>9090</td></tr>
<tr><td>MySQL</td><td>3306</td></tr>
<tr><td>PostgreSQL</td><td>5432</td></tr>
<tr><td>Mosquitto</td><td>1883</td></tr>
<tr><td>Kafka 1</td><td>19092</td></tr>
<tr><td>Kafka 2</td><td>29092</td></tr>
<tr><td>Kafka 3</td><td>39092</td></tr>
<tr><td>Schema Registry</td><td>8081</td></tr>
<tr><td>Kafka REST</td><td>8082</td></tr>
<tr><td>Kafka Connect</td><td>8083</td></tr>
<tr><td>Kafka MQTT</td><td>1884</td></tr>
<tr><td>ksqlDB</td><td>8088</td></tr>
<tr><td>Kafka Clients Spring Boot</td><td>8585</td></tr>
<tr><td>Kafka Streams gRPC Server</td><td>5050</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="about-this-book"><a class="header" href="#about-this-book">About This Book</a></h1>
<p>This book is power by <a href="https://rust-lang.github.io/mdBook/index.html">mdBook</a>.</p>
<p>GitHub <a href="https://github.com/sauljabin/kafka-sandbox">Repository</a>.</p>
<h3 id="developing-commands"><a class="header" href="#developing-commands">Developing Commands</a></h3>
<blockquote>
<p>You must install <a href="https://www.rust-lang.org/tools/install">rust</a> first.</p>
</blockquote>
<p>Install <code>mdbook</code>:</p>
<pre><code class="language-bash">cargo install mdbook
</code></pre>
<p>Run local server:</p>
<pre><code class="language-bash">mdbook serve --open
</code></pre>
<p>Build statics:</p>
<pre><code class="language-bash">mdbook build
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
