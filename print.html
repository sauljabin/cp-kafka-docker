<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Kafka Sandbox</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Setup</li><li class="chapter-item expanded "><a href="dependencies.html"><strong aria-hidden="true">1.</strong> Intall Dependencies</a></li><li class="chapter-item expanded "><a href="quick-start.html"><strong aria-hidden="true">2.</strong> Quick Start</a></li><li class="chapter-item expanded affix "><li class="part-title">Setting Up Some Tools</li><li class="chapter-item expanded "><a href="tools/kafka-cli-tools.html"><strong aria-hidden="true">3.</strong> Kafka CLI Tools</a></li><li class="chapter-item expanded "><a href="tools/sql-database.html"><strong aria-hidden="true">4.</strong> SQL Database</a></li><li class="chapter-item expanded "><a href="tools/sql-populate-database.html"><strong aria-hidden="true">5.</strong> SQL Populate Database</a></li><li class="chapter-item expanded "><a href="tools/nosql-database.html"><strong aria-hidden="true">6.</strong> NoSQL Database</a></li><li class="chapter-item expanded "><a href="tools/nosql-populate-database.html"><strong aria-hidden="true">7.</strong> NoSQL Populate Database</a></li><li class="chapter-item expanded "><a href="tools/mqtt-cli-tools.html"><strong aria-hidden="true">8.</strong> MQTT CLI Tools</a></li><li class="chapter-item expanded "><a href="tools/mqtt-broker.html"><strong aria-hidden="true">9.</strong> MQTT Broker</a></li><li class="chapter-item expanded affix "><li class="part-title">Using Kafka</li><li class="chapter-item expanded "><a href="using-kafka/kafka-cluster.html"><strong aria-hidden="true">10.</strong> Kafka Cluster</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-akhq.html"><strong aria-hidden="true">11.</strong> Kafka AKHQ</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-schema-registry.html"><strong aria-hidden="true">12.</strong> Kafka Schema Registry</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-connect/kafka-connect.html"><strong aria-hidden="true">13.</strong> Kafka Connect</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="using-kafka/kafka-connect/database-example.html"><strong aria-hidden="true">13.1.</strong> Database Example</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-connect/mqtt-example.html"><strong aria-hidden="true">13.2.</strong> MQTT Example</a></li></ol></li><li class="chapter-item expanded "><a href="using-kafka/kafka-ksqldb/kafka-ksqldb.html"><strong aria-hidden="true">14.</strong> Kafka ksqlDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="using-kafka/kafka-ksqldb/streams-example.html"><strong aria-hidden="true">14.1.</strong> ksqlDB Streams Example</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-ksqldb/extensions.html"><strong aria-hidden="true">14.2.</strong> Extensions</a></li></ol></li><li class="chapter-item expanded "><a href="using-kafka/kafka-clients/kafka-clients.html"><strong aria-hidden="true">15.</strong> Kafka Clients</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="using-kafka/kafka-clients/avro-producer-and-consumer.html"><strong aria-hidden="true">15.1.</strong> Avro Producer and Consumer</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-clients/spring-boot.html"><strong aria-hidden="true">15.2.</strong> Spring Boot</a></li></ol></li><li class="chapter-item expanded "><a href="using-kafka/kafka-streams/streams.html"><strong aria-hidden="true">16.</strong> Kafka Streams</a></li><li class="chapter-item expanded "><a href="using-kafka/kafka-performance-tools.html"><strong aria-hidden="true">17.</strong> Kafka Performance Tools</a></li><li class="chapter-item expanded affix "><li class="part-title">Proxies</li><li class="chapter-item expanded "><a href="proxies/kafka-rest-proxy.html"><strong aria-hidden="true">18.</strong> Kafka REST Proxy</a></li><li class="chapter-item expanded "><a href="proxies/kafka-mqtt-proxy.html"><strong aria-hidden="true">19.</strong> Kafka MQTT Proxy</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><a href="using-kafka/ports-table.html">Ports Table</a></li><li class="chapter-item expanded affix "><a href="book.html">About this book</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Kafka Sandbox</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/sauljabin/kafka-sandbox" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><strong>Kafka Sandbox</strong> helps you to deploy a kafka sandbox locally. It intends to be a simple way to get started with kafka and
help you on your learning path. It provides you with a wide variety of tools from the kafka ecosystem and a simple way
to run them all. It also includes a set of tools and tips to make it easier for you to use kafka. It does not include
security since it is not a production system.</p>
<h2 id="interesting-links"><a class="header" href="#interesting-links">Interesting Links</a></h2>
<ul>
<li><a href="https://developer.confluent.io/learn-kafka/">Confluent free courses</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/docker/image-reference.html">Confluent docker images references</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/versions-interoperability.html">Confluent versions interoperability</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intall-dependencies"><a class="header" href="#intall-dependencies">Intall Dependencies</a></h1>
<ul>
<li><a href="https://www.docker.com/">docker</a> - version 20 or higher</li>
<li><a href="https://docs.docker.com/compose/cli-command/">docker compose</a> - version 2 or higher</li>
<li><a href="https://www.java.com/en/download/">java</a> - version 11 or higher</li>
<li><a href="https://httpie.io/">httpie</a> - rest client</li>
<li><a href="https://stedolan.github.io/jq/">jq</a> - json parser</li>
</ul>
<h2 id="other-useful-utilities"><a class="header" href="#other-useful-utilities">Other Useful Utilities</a></h2>
<ul>
<li><a href="https://curl.se/">curl</a> - command line http client</li>
<li><a href="https://github.com/jesseduffield/lazydocker#installation">lazydocker</a> - docker text user interface</li>
<li><a href="https://github.com/sauljabin/kaskade#installation-and-usage">kaskade</a> - kafka text user interface</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>Clone the repo:</p>
<pre><code class="language-bash">git clone https://github.com/sauljabin/kafka-sandbox.git
cd kafka-sandbox
</code></pre>
<p>Create a docker network:</p>
<pre><code class="language-bash">docker network create kafka-sandbox_network
</code></pre>
<p>Run the kafka cluster:</p>
<pre><code class="language-bash">cd kafka-cluster
docker compose up -d
</code></pre>
<p>Run AKHQ:</p>
<pre><code class="language-bash">cd kafka-akhq
docker compose up -d
</code></pre>
<p>Open AKHQ at <a href="http://localhost:8080/">http://localhost:8080/</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-cli-tools"><a class="header" href="#kafka-cli-tools">Kafka CLI Tools</a></h1>
<p>It is a collection of tools to interact with kafka cluster through the terminal.</p>
<ul>
<li><a href="https://github.com/edenhill/kafkacat">kafkacat</a></li>
<li><a href="https://adevinta.github.io/zoe/">zoe</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/installing_cp/zip-tar.html">confluent community tools</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-cli">kafka-cli</a></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Create an alias for <code>kafka-cli</code>:</p>
<pre><code class="language-bash">alias kafka-cli='docker run --rm -it --network kafka-sandbox_network kafka-cli:latest '
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias kafka-cli='docker run --rm -it --network kafka-sandbox_network kafka-cli:latest '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Create the docker image:</p>
<pre><code class="language-bash">cd kafka-cli
docker build -t kafka-cli:latest .
kafka-cli
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sql-database"><a class="header" href="#sql-database">SQL Database</a></h1>
<p>Create a MySQL and PostgresSQL instance and a database.</p>
<ul>
<li><a href="https://hub.docker.com/_/mysql">mysql</a></li>
<li><a href="https://hub.docker.com/_/postgres">postgres</a></li>
<li><a href="https://hub.docker.com/_/adminer">adminer</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/sql-database">sql-database</a></li>
<li>postgres port: <code>5432</code></li>
<li>mysql port: <code>3306</code></li>
<li>adminer port: <code>9090</code> (<a href="http://localhost:9090/">open it in the web browser</a>)</li>
</ul>
<p>Run MySQL, PostgresSQL and Adminer:</p>
<pre><code class="language-bash">cd sql-database
docker compose up -d
</code></pre>
<h2 id="docker-compose"><a class="header" href="#docker-compose">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  mysql:
    image: mysql:8
    environment:
      TZ: America/Guayaquil
      MYSQL_DATABASE: sandbox
      MYSQL_ROOT_PASSWORD: notasecret
    ports:
      - 3306:3306
    restart: on-failure
    volumes:
      - mysql_data:/var/lib/mysql

  postgres:
    image: postgres:13
    environment:
      TZ: America/Guayaquil
      POSTGRES_DB: sandbox
      POSTGRES_PASSWORD: notasecret
    ports:
      - 5432:5432
    restart: on-failure
    volumes:
      - postgres_data:/var/lib/postgresql/data

  adminer:
    image: adminer:4
    environment:
      TZ: America/Guayaquil
    ports:
      - 9090:8080
    restart: on-failure

volumes:
  mysql_data:
  postgres_data:

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sql-populate-database"><a class="header" href="#sql-populate-database">SQL Populate Database</a></h1>
<p>This tool helps to populate either a MySQL or PostgresSQL database with random customers. This is an ancillary project
that can help us to set different scenarios.</p>
<ul>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/sql-populate">sql-populate</a></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Create an alias for <code>sql-populate</code>:</p>
<pre><code class="language-bash">alias sql-populate=&quot;$PWD/sql-populate/build/install/sql-populate/bin/sql-populate &quot;
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias sql-populate='$PWD/sql-populate/build/install/sql-populate/bin/sql-populate '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Install the app:</p>
<pre><code class="language-bash">./gradlew sql-populate:install
sql-populate
</code></pre>
<p>Examples:</p>
<pre><code class="language-bash">sql-populate --url &quot;jdbc:mysql://localhost:3306/sandbox&quot; --user &quot;root&quot; --password &quot;notasecret&quot; 100
sql-populate --url &quot;jdbc:postgresql://localhost:5432/sandbox&quot; --user &quot;postgres&quot; --password &quot;notasecret&quot; 100
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nosql-database"><a class="header" href="#nosql-database">NoSQL Database</a></h1>
<p>Create a MongoDB instance and a database.</p>
<ul>
<li><a href="https://hub.docker.com/_/mongo">mongo</a></li>
<li><a href="https://hub.docker.com/_/mongo-express">mongo express</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/nosql-database">nosql-database</a></li>
<li>mongo port: <code>27017</code></li>
<li>mongo express port: <code>7070</code> (<a href="http://localhost:7070/">open it in the web browser</a>)</li>
</ul>
<p>Run MongoDB and Mongo Express:</p>
<pre><code class="language-bash">cd nosql-database
docker compose up -d
</code></pre>
<h2 id="docker-compose-1"><a class="header" href="#docker-compose-1">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  mongo:
    image: mongo:5
    environment:
      TZ: America/Guayaquil
      MONGO_INITDB_ROOT_PASSWORD: notasecret
      MONGO_INITDB_ROOT_USERNAME: root
    ports:
      - 27017:27017
    restart: on-failure
    volumes:
      - mongo_data:/data/db
    healthcheck:
      test: echo 'db.runCommand(&quot;ping&quot;).ok' | mongo mongo:27017/test --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  mongo-express:
    image: mongo-express:latest
    environment:
      TZ: America/Guayaquil
      ME_CONFIG_MONGODB_ADMINPASSWORD: notasecret
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_PORT: &quot;27017&quot;
      ME_CONFIG_MONGODB_SERVER: mongo
    ports:
      - 7070:8081
    restart: on-failure
    depends_on:
      mongo:
        condition: service_healthy

volumes:
  mongo_data:

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nosql-populate-database"><a class="header" href="#nosql-populate-database">NoSQL Populate Database</a></h1>
<p>This tool helps to populate MongoDB with random customers. This is an ancillary project that can help us to set
different scenarios.</p>
<ul>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/nosql-populate">nosql-populate</a></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Create an alias for <code>nosql-populate</code>:</p>
<pre><code class="language-bash">alias nosql-populate=&quot;$PWD/nosql-populate/build/install/nosql-populate/bin/nosql-populate &quot;
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias nosql-populate='$PWD/nosql-populate/build/install/nosql-populate/bin/nosql-populate '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Install the app:</p>
<pre><code class="language-bash">./gradlew nosql-populate:install
nosql-populate
</code></pre>
<p>Example:</p>
<pre><code class="language-bash">nosql-populate --url &quot;mongodb://root:notasecret@localhost:27017&quot; -d &quot;sandbox&quot; 100
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mqtt-cli-tools"><a class="header" href="#mqtt-cli-tools">MQTT CLI Tools</a></h1>
<p>MQTT collection of tools to interact with a MQTT broker.</p>
<ul>
<li><a href="https://hivemq.github.io/mqtt-cli/">mqtt-cli</a></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Create an alias for <code>mqtt-cli</code>:</p>
<pre><code class="language-bash">alias mqtt-cli='docker run --rm -it --network kafka-sandbox_network hivemq/mqtt-cli:latest '
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias mqtt-cli='docker run --rm -it --network kafka-sandbox_network hivemq/mqtt-cli:latest '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Test the cli:</p>
<pre><code class="language-bash">mqtt-cli
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mqtt-broker"><a class="header" href="#mqtt-broker">MQTT Broker</a></h1>
<p>Eclipse Mosquitto is an open source (EPL/EDL licensed) message broker that implements the MQTT protocol versions 5.0,
3.1.1 and 3.1. Mosquitto is lightweight and is suitable for use on all devices from low power single board computers to
full servers.</p>
<ul>
<li><a href="https://mosquitto.org/">mosquitto</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/mqtt-broker">mqtt-broker</a></li>
<li>mosquitto port: <code>1883</code></li>
</ul>
<p>Run Mosquitto:</p>
<pre><code class="language-bash">cd mqtt-broker
docker compose up -d
mqtt-cli test -h mosquitto
</code></pre>
<h2 id="docker-compose-2"><a class="header" href="#docker-compose-2">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  mosquitto:
    image: eclipse-mosquitto:2
    environment:
      TZ: America/Guayaquil
    ports:
      - 1883:1883
    restart: on-failure
    volumes:
      - ./mosquitto.conf:/mosquitto/config/mosquitto.conf
      - mosquitto_data:/mosquitto/data

volumes:
  mosquitto_data:

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-cluster"><a class="header" href="#kafka-cluster">Kafka Cluster</a></h1>
<p>A three node kafka cluster.</p>
<ul>
<li><a href="https://kafka.apache.org/">kafka</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/configuration/broker-configs.html">kafka settings</a></li>
<li><a href="https://zookeeper.apache.org/">zookeeper</a></li>
<li><a href="https://docs.confluent.io/platform/current/zookeeper/deployment.html">zookeeper settings</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-cluster">kafka-cluster</a></li>
<li>kafka version: <a href="https://docs.confluent.io/platform/current/installation/versions-interoperability.html">cp 7.3.2</a></li>
<li>kafka ports: <code>19093</code>, <code>29093</code>, <code>39093</code></li>
<li>kafka jmx ports: <code>19999</code>, <code>29999</code>, <code>39999</code></li>
<li>zookeeper ports: <code>12181</code>, <code>22181</code>, <code>32181</code></li>
</ul>
<p>Run zookeeper and kafka:</p>
<pre><code class="language-bash">cd kafka-cluster
docker compose up -d
</code></pre>
<p>Test zookeeper and kafka (shows the broker ids):</p>
<pre><code class="language-bash">kafka-cli zookeeper-shell zookeeper1:2181 ls /brokers/ids
</code></pre>
<p>Create a topic:</p>
<pre><code class="language-bash">kafka-cli kafka-topics --create --bootstrap-server kafka1:9092 \
                       --replication-factor 3 \
                       --partitions 3 \
                       --topic kafka-cluster.test
kafka-cli kafka-topics --bootstrap-server kafka1:9092 --list
</code></pre>
<p>Produce a message:</p>
<pre><code class="language-bash">kafka-cli kafka-console-producer --broker-list kafka1:9092 --topic kafka-cluster.test
</code></pre>
<p>Consume messages:</p>
<pre><code class="language-bash">kafka-cli kafka-console-consumer --from-beginning --group kafka-cluster.test \
                                 --topic kafka-cluster.test  \
                                 --bootstrap-server kafka1:9092
</code></pre>
<blockquote>
<p>⚠️ The <code>JMX</code> ports were opened to monitor kafka using <code>jconsole</code>.</p>
</blockquote>
<p>Run <code>jconsole</code>:</p>
<pre><code class="language-bash">jconsole localhost:19999
</code></pre>
<h2 id="docker-compose-3"><a class="header" href="#docker-compose-3">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  kafka1:
    image: confluentinc/cp-kafka:${VERSION}
    environment:
      TZ: America/Guayaquil
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:9092,EXTERNAL://localhost:19093
      KAFKA_BROKER_ID: &quot;1&quot;
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:2181,zookeeper2:2181,zookeeper3:2181
      KAFKA_JMX_PORT: &quot;19999&quot;
    ports:
      - 19093:19093
      - 19999:19999
    restart: on-failure
    volumes:
      - kafka1_data:/var/lib/kafka/data
    depends_on:
      zookeeper1:
        condition: service_healthy
      zookeeper2:
        condition: service_healthy
      zookeeper3:
        condition: service_healthy
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:9092 --list &gt; /dev/null 2&gt;&amp;1
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s

  kafka2:
    image: confluentinc/cp-kafka:${VERSION}
    environment:
      TZ: America/Guayaquil
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:9092,EXTERNAL://localhost:29093
      KAFKA_BROKER_ID: &quot;2&quot;
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:2181,zookeeper2:2181,zookeeper3:2181
      KAFKA_JMX_PORT: &quot;29999&quot;
    ports:
      - 29093:29093
      - 29999:29999
    restart: on-failure
    volumes:
      - kafka2_data:/var/lib/kafka/data
    depends_on:
      zookeeper1:
        condition: service_healthy
      zookeeper2:
        condition: service_healthy
      zookeeper3:
        condition: service_healthy
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:9092 --list &gt; /dev/null 2&gt;&amp;1
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s

  kafka3:
    image: confluentinc/cp-kafka:${VERSION}
    environment:
      TZ: America/Guayaquil
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:9092,EXTERNAL://localhost:39093
      KAFKA_BROKER_ID: &quot;3&quot;
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:39093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:2181,zookeeper2:2181,zookeeper3:2181
      KAFKA_JMX_PORT: &quot;39999&quot;
    ports:
      - 39093:39093
      - 39999:39999
    restart: on-failure
    volumes:
      - kafka3_data:/var/lib/kafka/data
    depends_on:
      zookeeper1:
        condition: service_healthy
      zookeeper2:
        condition: service_healthy
      zookeeper3:
        condition: service_healthy
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:9092 --list &gt; /dev/null 2&gt;&amp;1
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s

  zookeeper1:
    image: confluentinc/cp-zookeeper:${VERSION}
    environment:
      TZ: America/Guayaquil
      ZOOKEEPER_CLIENT_PORT: &quot;2181&quot;
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888;zookeeper2:2888:3888;zookeeper3:2888:3888
      ZOOKEEPER_SERVER_ID: &quot;1&quot;
      ZOOKEEPER_SYNC_LIMIT: &quot;2&quot;
      ZOOKEEPER_TICK_TIME: &quot;2000&quot;
    ports:
      - 12181:2181
    restart: on-failure
    volumes:
      - zookeeper1_data:/var/lib/zookeeper/data
      - zookeeper1_datalog:/var/lib/zookeeper/log
    healthcheck:
      test: nc -z localhost 2181
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s

  zookeeper2:
    image: confluentinc/cp-zookeeper:${VERSION}
    environment:
      TZ: America/Guayaquil
      ZOOKEEPER_CLIENT_PORT: &quot;2181&quot;
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888;zookeeper2:2888:3888;zookeeper3:2888:3888
      ZOOKEEPER_SERVER_ID: &quot;2&quot;
      ZOOKEEPER_SYNC_LIMIT: &quot;2&quot;
      ZOOKEEPER_TICK_TIME: &quot;2000&quot;
    ports:
      - 22181:2181
    restart: on-failure
    volumes:
      - zookeeper2_data:/var/lib/zookeeper/data
      - zookeeper2_datalog:/var/lib/zookeeper/log
    healthcheck:
      test: nc -z localhost 2181
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s

  zookeeper3:
    image: confluentinc/cp-zookeeper:${VERSION}
    environment:
      TZ: America/Guayaquil
      ZOOKEEPER_CLIENT_PORT: &quot;2181&quot;
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888;zookeeper2:2888:3888;zookeeper3:2888:3888
      ZOOKEEPER_SERVER_ID: &quot;3&quot;
      ZOOKEEPER_SYNC_LIMIT: &quot;2&quot;
      ZOOKEEPER_TICK_TIME: &quot;2000&quot;
    ports:
      - 32181:2181
    restart: on-failure
    volumes:
      - zookeeper3_data:/var/lib/zookeeper/data
      - zookeeper3_datalog:/var/lib/zookeeper/log
    healthcheck:
      test: nc -z localhost 2181
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s

volumes:
  kafka1_data:
  kafka2_data:
  kafka3_data:
  zookeeper1_data:
  zookeeper1_datalog:
  zookeeper2_data:
  zookeeper2_datalog:
  zookeeper3_data:
  zookeeper3_datalog:

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-akhq"><a class="header" href="#kafka-akhq">Kafka AKHQ</a></h1>
<p>UI for managing kafka cluster.</p>
<ul>
<li><a href="https://akhq.io/">akhq</a></li>
<li><a href="https://github.com/tchiotludo/akhq#kafka-cluster-configuration">akhq settings</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-akhq">kafka-akhq</a></li>
<li>akhq port: <code>8080</code> (<a href="http://localhost:8080/">open it in the web browser</a>)</li>
</ul>
<p>Run AKHQ:</p>
<pre><code class="language-bash">cd kafka-akhq
docker compose up -d
</code></pre>
<h2 id="docker-compose-4"><a class="header" href="#docker-compose-4">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  akhq:
    image: tchiotludo/akhq:latest
    ports:
      - 8080:8080
    restart: on-failure
    healthcheck:
      test: curl http://localhost:8080
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 30s
    environment:
      TZ: America/Guayaquil
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: kafka1:9092,kafka2:9092,kafka3:9092
              schema-registry:
                url: http://schema-registry:8081
              connect:
                - name: kafka-connect
                  url: http://kafka-connect:8083

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-schema-registry"><a class="header" href="#kafka-schema-registry">Kafka Schema Registry</a></h1>
<p>It provides a RESTful interface for storing and retrieving your Avro, JSON Schema, and Protobuf schemas.</p>
<ul>
<li><a href="https://docs.confluent.io/platform/current/schema-registry/index.html">schema registry</a></li>
<li><a href="https://docs.confluent.io/platform/current/schema-registry/installation/config.html">schema registry settings</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-schema-registry">kafka-schema-registry</a></li>
<li>schema registry port: <code>8081</code></li>
</ul>
<p>Run Schema Registry:</p>
<pre><code class="language-bash">cd kafka-schema-registry
docker compose up -d
http :8081/config
</code></pre>
<h2 id="docker-compose-5"><a class="header" href="#docker-compose-5">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  schema-registry:
    image: confluentinc/cp-schema-registry:${VERSION}
    environment:
      TZ: America/Guayaquil
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: &quot;*&quot;
      SCHEMA_REGISTRY_DEBUG: &quot;true&quot;
      SCHEMA_REGISTRY_HOST_NAME: localhost
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: kafka-schema-registry.schemas
    ports:
      - 8081:8081
    restart: on-failure

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-connect"><a class="header" href="#kafka-connect">Kafka Connect</a></h1>
<p>It makes it simple to quickly define connectors that move large data sets into and out of Kafka.</p>
<ul>
<li><a href="https://docs.confluent.io/current/connect/index.html">connect</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/configuration/connect/index.html">connect settings</a></li>
<li><a href="https://docs.confluent.io/platform/current/connect/references/restapi.html">connect api reference</a></li>
<li><a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc">jdbc connector plugin</a></li>
<li><a href="https://www.confluent.io/hub/mongodb/kafka-connect-mongodb">mongo connector plugin</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-connect">kafka-connect</a></li>
<li>plugins location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-connect/plugins">kafka-connect/plugins</a></li>
<li>requests location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-connect/requests">kafka-connect/requests</a></li>
<li>connect port: <code>8083</code></li>
</ul>
<p>Run Kafka Connect:</p>
<pre><code class="language-bash">cd kafka-connect
docker compose up -d
http :8083/connector-plugins
</code></pre>
<h2 id="docker-compose-6"><a class="header" href="#docker-compose-6">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  kafka-connect:
    image: confluentinc/cp-kafka-connect:${VERSION}
    environment:
      TZ: America/Guayaquil
      CONNECT_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: localhost
      CONNECT_GROUP_ID: kafka-connect-sandbox
      CONNECT_PLUGIN_PATH: /usr/local/share/kafka/plugins
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect.config
      CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect.offsets
      CONNECT_STATUS_STORAGE_TOPIC: kafka-connect.status
    ports:
      - 8083:8083
    restart: on-failure
    volumes:
      - ./plugins:/usr/local/share/kafka/plugins

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-connect-database-example"><a class="header" href="#kafka-connect-database-example">Kafka Connect Database Example</a></h1>
<blockquote>
<p>⚠️ This example does not support deletion, for that you have to implement tombstone events at the <a href="https://debezium.io/documentation/reference/connectors/postgresql.html#postgresql-tombstone-events">source</a> and <a href="https://docs.confluent.io/kafka-connect-jdbc/current/sink-connector/index.html#jdbc-sink-delete-mode">sink</a>.</p>
</blockquote>
<p>Populate the databases:</p>
<pre><code class="language-bash">sql-populate --url &quot;jdbc:mysql://localhost:3306/sandbox&quot; --user &quot;root&quot; --password &quot;notasecret&quot; 100
</code></pre>
<p>Create the connectors using the API:</p>
<pre><code class="language-bash">cd kafka-connect
http :8083/connectors &lt; requests/create-connector-mysql-source.json
http :8083/connectors &lt; requests/create-connector-mongo-sink.json
http :8083/connectors &lt; requests/create-connector-postgres-sink.json
</code></pre>
<p>For deleting the connectors:</p>
<pre><code class="language-bash">http DELETE :8083/connectors/postgres-sink
http DELETE :8083/connectors/mongo-sink
http DELETE :8083/connectors/mysql-source
</code></pre>
<h2 id="requests"><a class="header" href="#requests">Requests</a></h2>
<h4 id="requestscreate-connector-mysql-sourcejson"><a class="header" href="#requestscreate-connector-mysql-sourcejson">requests/create-connector-mysql-source.json</a></h4>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;mysql-source&quot;,
  &quot;config&quot;: {
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;table.whitelist&quot;: &quot;customers&quot;,
    &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSourceConnector&quot;,
    &quot;connection.url&quot;: &quot;jdbc:mysql://mysql:3306/sandbox&quot;,
    &quot;connection.user&quot;: &quot;root&quot;,
    &quot;connection.password&quot;: &quot;notasecret&quot;,
    &quot;mode&quot;: &quot;timestamp&quot;,
    &quot;timestamp.column.name&quot;: &quot;created&quot;,
    &quot;topic.prefix&quot;: &quot;kafka-connect.&quot;,
    &quot;transforms&quot;: &quot;createKey&quot;,
    &quot;transforms.createKey.type&quot;: &quot;org.apache.kafka.connect.transforms.ValueToKey&quot;,
    &quot;transforms.createKey.fields&quot;: &quot;id&quot;
  }
}
</code></pre>
<h4 id="requestscreate-connector-mongo-sinkjson"><a class="header" href="#requestscreate-connector-mongo-sinkjson">requests/create-connector-mongo-sink.json</a></h4>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;mongo-sink&quot;,
  &quot;config&quot;: {
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;topics&quot;: &quot;kafka-connect.customers&quot;,
    &quot;collection&quot;: &quot;customers&quot;,
    &quot;connector.class&quot;: &quot;com.mongodb.kafka.connect.MongoSinkConnector&quot;,
    &quot;connection.uri&quot;: &quot;mongodb://root:notasecret@mongo:27017&quot;,
    &quot;database&quot;: &quot;sandbox&quot;
  }
}
</code></pre>
<h4 id="requestscreate-connector-postgres-sinkjson"><a class="header" href="#requestscreate-connector-postgres-sinkjson">requests/create-connector-postgres-sink.json</a></h4>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;postgres-sink&quot;,
  &quot;config&quot;: {
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSinkConnector&quot;,
    &quot;connection.url&quot;: &quot;jdbc:postgresql://postgres:5432/sandbox&quot;,
    &quot;connection.user&quot;: &quot;postgres&quot;,
    &quot;connection.password&quot;: &quot;notasecret&quot;,
    &quot;delete.enabled&quot;: false,
    &quot;pk.mode&quot;: &quot;record_key&quot;,
    &quot;pk.key&quot;: &quot;id&quot;,
    &quot;insert.mode&quot;: &quot;upsert&quot;,
    &quot;auto.create&quot;: true,
    &quot;topics&quot;: &quot;kafka-connect.customers&quot;,
    &quot;transforms&quot;: &quot;dropPrefix&quot;,
    &quot;transforms.dropPrefix.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
    &quot;transforms.dropPrefix.regex&quot;: &quot;kafka-connect\\.(.*)&quot;,
    &quot;transforms.dropPrefix.replacement&quot;: &quot;$1&quot;
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-connect-mqtt-example"><a class="header" href="#kafka-connect-mqtt-example">Kafka Connect MQTT Example</a></h1>
<p>Subscribe to topics (for debugging purposes):</p>
<pre><code class="language-bash">mqtt-cli sub -h mosquitto -t 'house/+/brightness'
</code></pre>
<p>Create a connector using the API:</p>
<pre><code class="language-bash">cd kafka-connect
http :8083/connectors &lt; requests/create-connector-mqtt-source.json
</code></pre>
<p>Publish messages:</p>
<pre><code class="language-bash">mqtt-cli pub -h mosquitto -t 'house/room/brightness' -m '800LM'
mqtt-cli pub -h mosquitto -t 'house/kitchen/brightness' -m '1000LM'
</code></pre>
<p>Consuming the data:</p>
<pre><code class="language-bash">kafka-cli kafka-console-consumer --from-beginning --group kafka-connect.brightness_consumer \
                                 --topic kafka-connect.brightness  \
                                 --bootstrap-server kafka1:9092 \
                                 --property print.key=true
</code></pre>
<p>For deleting the connector:</p>
<pre><code class="language-bash">http DELETE :8083/connectors/mqtt-source
</code></pre>
<h2 id="requests-1"><a class="header" href="#requests-1">Requests</a></h2>
<h4 id="requestscreate-connector-mqtt-sourcejson"><a class="header" href="#requestscreate-connector-mqtt-sourcejson">requests/create-connector-mqtt-source.json</a></h4>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;mqtt-source&quot;,
  &quot;config&quot;: {
      &quot;connector.class&quot;: &quot;io.confluent.connect.mqtt.MqttSourceConnector&quot;,
      &quot;tasks.max&quot;: &quot;1&quot;,
      &quot;mqtt.server.uri&quot;: &quot;tcp://mosquitto:1883&quot;,
      &quot;mqtt.topics&quot;:&quot;house/+/brightness&quot;,
      &quot;kafka.topic&quot;:&quot;kafka-connect.brightness&quot;,
      &quot;mqtt.qos&quot;: &quot;2&quot;,
      &quot;confluent.topic.bootstrap.servers&quot;: &quot;kafka1:9092&quot;,
      &quot;confluent.topic.replication.factor&quot;: &quot;1&quot;
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-ksqldb"><a class="header" href="#kafka-ksqldb">Kafka ksqlDB</a></h1>
<p>ksqlDB is a database that's purpose-built for stream processing applications.</p>
<blockquote>
<p>⚠️ ksqlDB it is not a SQL database, it provides an extra layer for implementing kstream, ktable and connectors through a language (ksql) based on sql.</p>
</blockquote>
<ul>
<li><a href="https://ksqldb.io/">ksqldb</a></li>
<li><a href="https://docs.ksqldb.io/en/latest/reference/server-configuration/">ksqldb settings</a></li>
<li><a href="https://docs.ksqldb.io/en/latest/how-to-guides/test-an-app/">ksqldb test runner</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-ksqldb">kafka-ksqldb</a></li>
<li>ksqldb port: <code>8088</code></li>
</ul>
<p>Create an alias for <code>ksqldb-cli</code>:</p>
<blockquote>
<p>⚠️ Run alias commands inside the root folder.</p>
</blockquote>
<pre><code class="language-bash">alias ksqldb-cli=&quot;docker run --rm -it --network kafka-sandbox_network --workdir /ksqldb -v $PWD/kafka-ksqldb/tests:/ksqldb/tests -v $PWD/kafka-ksqldb/statements:/ksqldb/statements -v $PWD/kafka-ksqldb-extensions/extensions:/ksqldb/extensions kafka-cli:latest &quot;
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias ksqldb-cli='docker run --rm -it --network kafka-sandbox_network --workdir /ksqldb -v $PWD/kafka-ksqldb/tests:/ksqldb/tests -v $PWD/kafka-ksqldb/statements:/ksqldb/statements -v $PWD/kafka-ksqldb-extensions/extensions:/ksqldb/extensions kafka-cli:latest '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Run ksqlDB:</p>
<pre><code class="language-bash">cd kafka-ksqldb
docker compose up -d
http :8088/info
</code></pre>
<p>One line shell interaction:</p>
<pre><code class="language-bash">ksqldb-cli ksql -e &quot;SHOW STREAMS;&quot; http://ksqldb:8088
</code></pre>
<p>Interactive ksqlDB shell:</p>
<pre><code class="language-bash">ksqldb-cli ksql http://ksqldb:8088
SHOW STREAMS;
</code></pre>
<h2 id="docker-compose-7"><a class="header" href="#docker-compose-7">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  ksqldb:
    image: confluentinc/cp-ksqldb-server:${VERSION}
    environment:
      TZ: America/Guayaquil
      KSQL_KSQL_SERVICE_ID: kafka-ksqldb.
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: &quot;true&quot;
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: &quot;true&quot;
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      KSQL_KSQL_CONNECT_URL: http://kafka-connect:8083
      KSQL_KSQL_EXTENSION_DIR: /ksqldb/extensions
    ports:
      - 8088:8088
    restart: on-failure
    volumes:
      - ../kafka-ksqldb-extensions/extensions:/ksqldb/extensions

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ksqldb-streams-example"><a class="header" href="#ksqldb-streams-example">ksqlDB Streams Example</a></h1>
<ul>
<li>statements location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-ksqldb/statements">kafka-ksqldb/statements</a></li>
<li>test location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-ksqldb/tests">kafka-ksqldb/tests</a></li>
</ul>
<p>Test runner:</p>
<pre><code class="language-bash">ksqldb-cli ksql-test-runner -e extensions/ \
                            -s statements/create-orders.ksql \
                            -i tests/orders-input.json \
                            -o tests/orders-output.json | grep '&gt;&gt;&gt;'
</code></pre>
<p>Execute statement files:</p>
<pre><code class="language-bash">ksqldb-cli ksql -f statements/create-orders.ksql http://ksqldb:8088
ksqldb-cli ksql -f statements/insert-orders.ksql http://ksqldb:8088
</code></pre>
<p>List of streams:</p>
<pre><code class="language-bash">http :8088/ksql ksql=&quot;list streams;&quot; | jq '.[].streams[] | [{name: .name, topic: .topic}]'
</code></pre>
<p>Show content:</p>
<pre><code class="language-bash">ksqldb-cli ksql -e &quot;PRINT 'kafka-ksqldb.order_sizes' FROM BEGINNING;&quot; http://ksqldb:8088
</code></pre>
<p>Deleting all orders:</p>
<pre><code class="language-bash">ksqldb-cli ksql -e &quot;DROP STREAM ORDERSIZES DELETE TOPIC; DROP STREAM ORDERS DELETE TOPIC;&quot; http://ksqldb:8088
</code></pre>
<h2 id="statements"><a class="header" href="#statements">Statements</a></h2>
<h4 id="statementscreate-ordersksql"><a class="header" href="#statementscreate-ordersksql">statements/create-orders.ksql</a></h4>
<pre><code class="language-sql">CREATE STREAM orders (orderId INT KEY, orderUnits INT, totalAmount DOUBLE)
  WITH (kafka_topic='kafka-ksqldb.orders', key_format='json', value_format='json', partitions=10, replicas=3);

CREATE STREAM orderSizes 
  WITH (kafka_topic='kafka-ksqldb.order_sizes', key_format='json', value_format='json', partitions=10, replicas=3)
  AS SELECT
    orderId,
    orderUnits,
    totalAmount,
    CASE WHEN orderUnits &lt; 2 THEN 'small' WHEN orderUnits &lt; 4 THEN 'medium' ELSE 'large' END AS orderSize,
    taxes(totalAmount) AS tax
  FROM orders EMIT CHANGES;
</code></pre>
<h4 id="statementsinsert-ordersksql"><a class="header" href="#statementsinsert-ordersksql">statements/insert-orders.ksql</a></h4>
<pre><code class="language-sql">INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000000, 2, 100.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000001, 4, 200.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000002, 6, 300.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000003, 3, 150.0);
INSERT INTO orders (orderId, orderUnits, totalAmount) VALUES (1000004, 1, 50.0);
</code></pre>
<h2 id="tests"><a class="header" href="#tests">Tests</a></h2>
<h4 id="testsorders-inputjson"><a class="header" href="#testsorders-inputjson">tests/orders-input.json</a></h4>
<pre><code class="language-json">{
  &quot;inputs&quot;: [
    {&quot;topic&quot;: &quot;kafka-ksqldb.orders&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000000, &quot;value&quot;: {&quot;orderUnits&quot;: 2, &quot;totalAmount&quot;: 100.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.orders&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000001, &quot;value&quot;: {&quot;orderUnits&quot;: 4, &quot;totalAmount&quot;: 200.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.orders&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000002, &quot;value&quot;: {&quot;orderUnits&quot;: 6, &quot;totalAmount&quot;: 300.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.orders&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000003, &quot;value&quot;: {&quot;orderUnits&quot;: 3, &quot;totalAmount&quot;: 150.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.orders&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000004, &quot;value&quot;: {&quot;orderUnits&quot;: 1, &quot;totalAmount&quot;: 50.0}}
  ]
}
</code></pre>
<h4 id="testsorders-outputjson"><a class="header" href="#testsorders-outputjson">tests/orders-output.json</a></h4>
<pre><code class="language-json">{
  &quot;outputs&quot;: [
    {&quot;topic&quot;: &quot;kafka-ksqldb.order_sizes&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000000, &quot;value&quot;: {&quot;ORDERUNITS&quot;: 2, &quot;TOTALAMOUNT&quot;: 100.0, &quot;ORDERSIZE&quot;: &quot;medium&quot;, &quot;TAX&quot;: 12.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.order_sizes&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000001, &quot;value&quot;: {&quot;ORDERUNITS&quot;: 4, &quot;TOTALAMOUNT&quot;: 200.0, &quot;ORDERSIZE&quot;: &quot;large&quot; , &quot;TAX&quot;: 24.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.order_sizes&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000002, &quot;value&quot;: {&quot;ORDERUNITS&quot;: 6, &quot;TOTALAMOUNT&quot;: 300.0, &quot;ORDERSIZE&quot;: &quot;large&quot; , &quot;TAX&quot;: 36.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.order_sizes&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000003, &quot;value&quot;: {&quot;ORDERUNITS&quot;: 3, &quot;TOTALAMOUNT&quot;: 150.0, &quot;ORDERSIZE&quot;: &quot;medium&quot;, &quot;TAX&quot;: 18.0}},
    {&quot;topic&quot;: &quot;kafka-ksqldb.order_sizes&quot;, &quot;timestamp&quot;: 0, &quot;key&quot;: 1000004, &quot;value&quot;: {&quot;ORDERUNITS&quot;: 1, &quot;TOTALAMOUNT&quot;: 50.0, &quot;ORDERSIZE&quot;: &quot;small&quot; , &quot;TAX&quot;: 6.0}}
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ksqldb-extensions"><a class="header" href="#ksqldb-extensions">ksqlDB Extensions</a></h1>
<p>ksqlDB extensions are pieces of logic for transforming or aggregating events that ksqlDB can't currently express.</p>
<ul>
<li><a href="https://docs.ksqldb.io/en/latest/how-to-guides/create-a-user-defined-function">ksqldb extensions (udf, udtf, udaf)</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-ksqldb-extensions">kafka-ksqldb-extensions</a></li>
<li>extensions location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-ksqldb-extensions/extensions">kafka-ksqldb-extensions/extensions</a></li>
</ul>
<p>Check the <a href="using-kafka/kafka-ksqldb/./kafka-ksqldb.html">Kafka ksqlDB</a> section.</p>
<p>For creating the <code>jar</code> extension, you can use the following command (development purposes):</p>
<pre><code class="language-bash">./gradlew kafka-ksqldb-extensions:shadowJar
</code></pre>
<h2 id="custom-udf"><a class="header" href="#custom-udf">Custom UDF</a></h2>
<pre><code class="language-java">package kafka.sandbox.ksqldb;

import io.confluent.ksql.function.udf.Udf;
import io.confluent.ksql.function.udf.UdfDescription;
import io.confluent.ksql.function.udf.UdfParameter;

@UdfDescription(
    name = &quot;taxes&quot;,
    author = &quot;kafka sandbox&quot;,
    version = &quot;1.0.0&quot;,
    description = &quot;A custom taxes formula for orders.&quot;
)
public class TaxesUdf {

    public static final double TAXES = .12;

    @Udf(description = &quot;Calculate taxes.&quot;)
    public double taxes(@UdfParameter double amount) {
        return amount * TAXES;
    }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-clients"><a class="header" href="#kafka-clients">Kafka Clients</a></h1>
<p>Java examples for producing and consuming messages from Kafka using the
<a href="https://docs.confluent.io/clients-kafka-java/current/overview.html">java kafka client</a> lib.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="avro-producer-and-consumer"><a class="header" href="#avro-producer-and-consumer">Avro Producer and Consumer</a></h1>
<p>These examples produce and consume messages from the <code>supplier</code> topic. The producer example produces random suppliers.</p>
<ul>
<li><a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-avro.html">kafka producer and consumer example</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html">kafka consumer settings</a></li>
<li><a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html">kafka producer settings</a></li>
<li>avro project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-avro">kafka-avro</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-clients">kafka-clients</a></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Create an alias for <code>kafka-clients</code>:</p>
<pre><code class="language-bash">alias kafka-clients=&quot;$PWD/kafka-clients/build/install/kafka-clients/bin/kafka-clients &quot;
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias kafka-clients='$PWD/kafka-clients/build/install/kafka-clients/bin/kafka-clients '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Create a topic:</p>
<pre><code class="language-bash">kafka-cli kafka-topics --create --bootstrap-server kafka1:9092 \
                       --replication-factor 3 \
                       --partitions 3 \
                       --topic kafka-clients.suppliers
</code></pre>
<p>Install the app:</p>
<pre><code class="language-bash">./gradlew kafka-clients:install
kafka-clients
</code></pre>
<p>Run clients:</p>
<pre><code class="language-bash">kafka-clients producer 100
kafka-clients consumer
</code></pre>
<p>For creating a AVRO schema, you can use the following command (development purposes):</p>
<pre><code class="language-bash">./gradlew kafka-avro:build
</code></pre>
<h2 id="avro-schema"><a class="header" href="#avro-schema">Avro Schema</a></h2>
<h4 id="suppliers-v1avsc"><a class="header" href="#suppliers-v1avsc">suppliers-v1.avsc</a></h4>
<pre><code class="language-json">{
  &quot;type&quot;: &quot;record&quot;,
  &quot;name&quot;: &quot;Supplier&quot;,
  &quot;namespace&quot;: &quot;kafka.sandbox.avro&quot;,
  &quot;version&quot;: &quot;1&quot;,
  &quot;fields&quot;: [
    {
      &quot;name&quot;: &quot;id&quot;,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;name&quot;: &quot;name&quot;,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;name&quot;: &quot;address&quot;,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;name&quot;: &quot;country&quot;,
      &quot;type&quot;: &quot;string&quot;
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spring-boot"><a class="header" href="#spring-boot">Spring Boot</a></h1>
<p>Spring Boot + Spring Kafka producer and consumer examples.</p>
<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-spring-boot-application/">confluent spring kafka examples</a></li>
<li><a href="https://docs.spring.io/spring-kafka/reference/html/">spring kafka settings</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-spring-boot">kafka-spring-boot</a></li>
<li>spring port: <code>8585</code></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Run spring boot:</p>
<pre><code class="language-bash">./gradlew kafka-spring-boot:bootRun
</code></pre>
<p>In another terminal:</p>
<pre><code class="language-bash">http :8585/actuator/health
http :8585/produce messages==10
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="streams"><a class="header" href="#streams">Streams</a></h1>
<p>Kafka Streams is a client library providing organizations with a particularly efficient framework for processing
streaming data. It offers a streamlined method for creating applications and microservices that must process data in
real-time to be effective.</p>
<p>Check the <a href="using-kafka/kafka-streams/../kafka-clients/avro-producer-and-consumer.html">Kafka Clients - Avro Producer and Consumer</a> section.</p>
<ul>
<li><a href="https://kafka.apache.org/documentation/streams/">kafka streams</a></li>
<li><a href="https://github.com/confluentinc/kafka-streams-examples">kafka streams examples</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-streams">kafka-streams</a></li>
</ul>
<blockquote>
<p>⚠️ Run these commands inside the root folder.</p>
</blockquote>
<p>Create an alias for <code>kafka-streams</code>:</p>
<pre><code class="language-bash">alias kafka-streams=&quot;$PWD/kafka-streams/build/install/kafka-streams/bin/kafka-streams &quot;
</code></pre>
<p>To permanently add the alias to your shell (<code>~/.bashrc</code> or <code>~/.zshrc</code> file):</p>
<pre><code class="language-bash">echo &quot;alias kafka-streams='$PWD/kafka-streams/build/install/kafka-streams/bin/kafka-streams '&quot; &gt;&gt; ~/.zshrc
</code></pre>
<p>Install the app:</p>
<pre><code class="language-bash">./gradlew kafka-streams:install
kafka-streams
</code></pre>
<p>Run streams:</p>
<pre><code class="language-bash">kafka-streams streams
</code></pre>
<p>Print results:</p>
<pre><code class="language-bash">kafka-cli kafka-console-consumer --from-beginning --group kafka-streams.consumer \
                                 --topic kafka-streams.supplier_counts_by_country  \
                                 --bootstrap-server kafka1:9092 \
                                 --property print.key=true \
                                 --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-performance-tools"><a class="header" href="#kafka-performance-tools">Kafka Performance Tools</a></h1>
<p>Performance tuning involves two important metrics:</p>
<ul>
<li>Latency measures how long it takes to process one event.</li>
<li>Throughput measures how many events arrive within a specific amount of time.</li>
</ul>
<p>Run help:</p>
<pre><code class="language-bash">kafka-cli kafka-producer-perf-test --help
kafka-cli kafka-consumer-perf-test --help
</code></pre>
<p>Create a topic:</p>
<pre><code class="language-bash">kafka-cli kafka-topics --create --bootstrap-server kafka1:9092 \
                       --replication-factor 3 \
                       --partitions 3 \
                       --topic kafka-cluster.performance-test
</code></pre>
<h2 id="performance-tests"><a class="header" href="#performance-tests">Performance Tests</a></h2>
<p>Test producer:</p>
<pre><code class="language-bash">kafka-cli kafka-producer-perf-test --topic kafka-cluster.performance-test \
                                   --throughput -1 \
                                   --num-records 3000000 \
                                   --record-size 1024 \
                                   --producer-props acks=all bootstrap.servers=kafka1:9092,kafka2:9092,kafka3:9092
</code></pre>
<ul>
<li>Throughput in MB/sec.</li>
<li>Latency in miliseconds.</li>
</ul>
<p>Test consumer:</p>
<pre><code class="language-bash">kafka-cli kafka-consumer-perf-test --topic kafka-cluster.performance-test \
                                   --broker-list kafka1:9092,kafka2:9092,kafka3:9092 \
                                   --messages 3000000
</code></pre>
<ul>
<li><code>start.time, end.time</code>: shows test start and end time.</li>
<li><code>data.consumed.in.MB</code>: shows the size of all messages consumed.</li>
<li><code>MB.sec</code>: shows how much data transferred in megabytes per second (Throughput on size).</li>
<li><code>data.consumed.in.nMsg</code>: shows the count of the total messages consumed during this test.</li>
<li><code>nMsg.sec</code>: shows how many messages were consumed in a second (Throughput on the count of messages).</li>
</ul>
<p>Test end to end latency:</p>
<pre><code class="language-bash">kafka-cli kafka-run-class kafka.tools.EndToEndLatency kafka1:9092,kafka2:9092,kafka3:9092 kafka-cluster.performance-test 10000 1 1024
</code></pre>
<ul>
<li>This class records the average end to end latency for a single message to travel through Kafka.</li>
<li>Latency in miliseconds.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-rest-proxy"><a class="header" href="#kafka-rest-proxy">Kafka REST Proxy</a></h1>
<p>The Kafka REST Proxy provides a RESTful interface to a Kafka cluster.</p>
<blockquote>
<p>⚠️ Use this when you really need a rest interface since it is usually more complex than using conventional kafka clients.</p>
</blockquote>
<ul>
<li><a href="https://docs.confluent.io/platform/current/kafka-rest/index.html">kafka rest</a></li>
<li><a href="https://docs.confluent.io/platform/current/kafka-rest/production-deployment/rest-proxy/config.html">kafka rest settings</a></li>
<li><a href="https://docs.confluent.io/platform/current/kafka-rest/api.html">kafka rest api reference</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-rest">kafka-rest</a></li>
<li>requests location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-rest/requests">kafka-rest/requests</a></li>
<li>kafka rest port: <code>8082</code></li>
</ul>
<p>Run Kafka REST Proxy:</p>
<pre><code class="language-bash">cd kafka-rest
docker compose up -d
http :8082/brokers
</code></pre>
<p>Create topics:</p>
<pre><code class="language-bash">cd kafka-rest
http :8082/topics/kafka-rest.test Content-Type:application/vnd.kafka.json.v2+json records:='[{ &quot;key&quot;: &quot;test&quot;, &quot;value&quot;: &quot;test&quot; }]'
http :8082/topics/kafka-rest.users Content-Type:application/vnd.kafka.avro.v2+json &lt; requests/produce-avro-message.json
</code></pre>
<h2 id="docker-compose-8"><a class="header" href="#docker-compose-8">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  kafka-rest:
    image: confluentinc/cp-kafka-rest:${VERSION}
    environment:
      TZ: America/Guayaquil
      KAFKA_REST_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
      KAFKA_REST_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    ports:
      - 8082:8082
    restart: on-failure

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<h2 id="requests-2"><a class="header" href="#requests-2">Requests</a></h2>
<h4 id="requestsproduce-avro-messagejson"><a class="header" href="#requestsproduce-avro-messagejson">requests/produce-avro-message.json</a></h4>
<pre><code class="language-json">{
  &quot;value_schema&quot;: &quot;{\&quot;type\&quot;: \&quot;record\&quot;, \&quot;name\&quot;: \&quot;User\&quot;, \&quot;fields\&quot;: [{\&quot;name\&quot;: \&quot;name\&quot;, \&quot;type\&quot;: \&quot;string\&quot;}]}&quot;,
  &quot;records&quot;: [
    {
      &quot;value&quot;: {
        &quot;name&quot;: &quot;John Doe&quot;
      }
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kafka-mqtt-proxy"><a class="header" href="#kafka-mqtt-proxy">Kafka MQTT Proxy</a></h1>
<p>MQTT Proxy enables MQTT clients to use the MQTT 3.1.1 protocol to publish data directly to Apache Kafka.</p>
<blockquote>
<p>⚠️ This does not convert kafka into a MQTT broker, this aims to provide a simple way to publish/persist IoT data to Kafka.</p>
</blockquote>
<ul>
<li><a href="https://docs.confluent.io/platform/current/kafka-mqtt/intro.html">kafka mqtt</a></li>
<li><a href="https://docs.confluent.io/platform/current/kafka-mqtt/configuration_options.html">kafka mqtt settings</a></li>
<li>project location: <a href="https://github.com/sauljabin/kafka-sandbox/tree/main/kafka-mqtt">kafka-mqtt</a></li>
<li>kafka mqtt tcp port: <code>1884</code></li>
</ul>
<p>Run Kafka MQTT Proxy:</p>
<pre><code class="language-bash">cd kafka-mqtt
docker compose up -d
</code></pre>
<p>Publish a message:</p>
<pre><code class="language-bash">mqtt-cli pub -h kafka-mqtt -p 1884 -t 'house/room/temperature' -m '20C'
</code></pre>
<p>Consuming the data:</p>
<pre><code class="language-bash">kafka-cli kafka-console-consumer --from-beginning --group kafka-mqtt.consumer \
                                 --topic kafka-mqtt.temperature  \
                                 --bootstrap-server kafka1:9092 \
                                 --property print.key=true
</code></pre>
<h2 id="docker-compose-9"><a class="header" href="#docker-compose-9">Docker Compose</a></h2>
<pre><code class="language-yaml">version: &quot;3.9&quot;

services:
  kafka-mqtt:
    image: confluentinc/cp-kafka-mqtt:${VERSION}
    environment:
      TZ: America/Guayaquil
      KAFKA_MQTT_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
      KAFKA_MQTT_TOPIC_REGEX_LIST: kafka-mqtt.temperature:.*temperature
      KAFKA_MQTT_LISTENERS: 0.0.0.0:1884
    ports:
      - 1884:1884
    restart: on-failure

networks:
  default:
    external: true
    name: kafka-sandbox_network
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ui-ports-table"><a class="header" href="#ui-ports-table">UI Ports Table</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Service</th><th>Host</th><th>Port</th></tr></thead><tbody>
<tr><td>AKHQ</td><td>localhost</td><td><a href="http://localhost:8080/">8080</a></td></tr>
<tr><td>Adminer</td><td>localhost</td><td><a href="http://localhost:9090/">9090</a></td></tr>
<tr><td>Mongo Express</td><td>localhost</td><td><a href="http://localhost:7070/">7070</a></td></tr>
</tbody></table>
</div>
<h1 id="component-ports-table"><a class="header" href="#component-ports-table">Component Ports Table</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Service</th><th>Host</th><th>Port</th></tr></thead><tbody>
<tr><td>MySQL</td><td>mysql</td><td>3306</td></tr>
<tr><td>MySQL</td><td>localhost</td><td>3306</td></tr>
<tr><td>PostgreSQL</td><td>postgres</td><td>5432</td></tr>
<tr><td>PostgreSQL</td><td>localhost</td><td>5432</td></tr>
<tr><td>MongoDB</td><td>mongo</td><td>27017</td></tr>
<tr><td>MongoDB</td><td>localhost</td><td>27017</td></tr>
<tr><td>Mosquitto</td><td>mosquitto</td><td>1883</td></tr>
<tr><td>Mosquitto</td><td>localhost</td><td>1883</td></tr>
<tr><td>Kafka 1</td><td>kafka1</td><td>9092</td></tr>
<tr><td>Kafka 1</td><td>localhost</td><td>19093</td></tr>
<tr><td>Kafka 2</td><td>kafka2</td><td>9092</td></tr>
<tr><td>Kafka 2</td><td>localhost</td><td>29093</td></tr>
<tr><td>Kafka 3</td><td>kafka3</td><td>9092</td></tr>
<tr><td>Kafka 3</td><td>localhost</td><td>39093</td></tr>
<tr><td>Kafka 1 JMX</td><td>kafka1</td><td>19999</td></tr>
<tr><td>Kafka 1 JMX</td><td>localhost</td><td>19999</td></tr>
<tr><td>Kafka 2 JMX</td><td>kafka2</td><td>29999</td></tr>
<tr><td>Kafka 2 JMX</td><td>localhost</td><td>29999</td></tr>
<tr><td>Kafka 3 JMX</td><td>kafka3</td><td>39999</td></tr>
<tr><td>Kafka 3 JMX</td><td>localhost</td><td>39999</td></tr>
<tr><td>Zookeeper 1</td><td>zookeeper1</td><td>2181</td></tr>
<tr><td>Zookeeper 1</td><td>localhost</td><td>12181</td></tr>
<tr><td>Zookeeper 2</td><td>zookeeper2</td><td>2181</td></tr>
<tr><td>Zookeeper 2</td><td>localhost</td><td>22181</td></tr>
<tr><td>Zookeeper 3</td><td>zookeeper3</td><td>2181</td></tr>
<tr><td>Zookeeper 3</td><td>localhost</td><td>32181</td></tr>
<tr><td>Schema Registry</td><td>schema-registry</td><td>8081</td></tr>
<tr><td>Schema Registry</td><td>localhost</td><td>8081</td></tr>
<tr><td>Kafka REST</td><td>kafka-rest</td><td>8082</td></tr>
<tr><td>Kafka REST</td><td>localhost</td><td>8082</td></tr>
<tr><td>Kafka Connect</td><td>kafka-connect</td><td>8083</td></tr>
<tr><td>Kafka Connect</td><td>localhost</td><td>8083</td></tr>
<tr><td>Kafka MQTT</td><td>kafka-mqtt</td><td>1884</td></tr>
<tr><td>Kafka MQTT</td><td>localhost</td><td>1884</td></tr>
<tr><td>ksqlDB</td><td>ksqldb</td><td>8088</td></tr>
<tr><td>ksqlDB</td><td>localhost</td><td>8088</td></tr>
<tr><td>Kafka Clients Spring Boot</td><td>localhost</td><td>8585</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="about-this-book"><a class="header" href="#about-this-book">About This Book</a></h1>
<p>This book is power by <a href="https://rust-lang.github.io/mdBook/index.html">mdBook</a>.</p>
<p>GitHub <a href="https://github.com/sauljabin/kafka-sandbox">Repository</a>.</p>
<h2 id="developing-commands"><a class="header" href="#developing-commands">Developing Commands</a></h2>
<blockquote>
<p>You must install <a href="https://www.rust-lang.org/tools/install">rust</a> first.</p>
</blockquote>
<p>Install <code>mdbook</code>:</p>
<pre><code class="language-bash">cargo install mdbook
</code></pre>
<p>Run local server:</p>
<pre><code class="language-bash">mdbook serve --open
</code></pre>
<p>Build statics:</p>
<pre><code class="language-bash">mdbook build
</code></pre>
<h2 id="using-docker"><a class="header" href="#using-docker">Using Docker</a></h2>
<p>Create docker image:</p>
<pre><code class="language-bash">docker build -t sauljabin/kafka-sandbox-book:latest -f docker/Dockerfile .
</code></pre>
<p>Running the book (<a href="http://localhost">open it in the web browser</a>):</p>
<pre><code class="language-bash">docker run --name kafka-sandbox-book -d -p 80:80 sauljabin/kafka-sandbox-book:latest
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
